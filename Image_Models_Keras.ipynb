{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Image_Models_Keras.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1k7KsHBNTQTn3Ab_-oCnygik2Ll_YLcb7",
      "authorship_tag": "ABX9TyOnViA6y2p3zOsurSHIxT5G"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "aEdCbGxTrPG3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#https://github.com/devforfu/Models/blob/master/dogsbreeds/dogsbreeds.ipynb\n",
        "#https://keras.io/api/applications/\n",
        "#https://marubon-ds.blogspot.com/2018/03/some-fine-tuning-models-with-keras.html"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zfSa7mqQP4LK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pip install import-ipynb"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XDAdj3hAvtSi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "from keras.datasets import cifar10\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Model\n",
        "from keras.layers import Dense, GlobalAveragePooling2D\n",
        "from keras.optimizers import SGD\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.applications import InceptionV3, VGG16, DenseNet201, MobileNetV2, ResNet152V2, InceptionResNetV2, NASNetLarge, Xception\n",
        "from matplotlib import pyplot\n",
        "\n",
        "%cd \"/content/drive/My Drive/Dog_Whistle_Code\"\n",
        "from HelperFunctions import *"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2ZJ16QatF_2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# read data\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data() #TODO: add dog whistle data\n",
        "\n",
        "# limit the amount of the data\n",
        "# train data\n",
        "ind_train = random.sample(list(range(x_train.shape[0])), 1000)\n",
        "x_train = x_train[ind_train]\n",
        "y_train = y_train[ind_train]\n",
        "\n",
        "# test data\n",
        "ind_test = random.sample(list(range(x_test.shape[0])), 1000)\n",
        "x_test = x_test[ind_test]\n",
        "y_test = y_test[ind_test]\n",
        "\n",
        "def resize_data(data):\n",
        "    \"\"\"\n",
        "    Resize data to be compatible with model inputs\n",
        "\n",
        "    data: image data\n",
        "    \"\"\"\n",
        "    data_upscaled = np.zeros((data.shape[0], 320, 320, 3))\n",
        "    for i, img in enumerate(data):\n",
        "        large_img = cv2.resize(img, dsize=(320, 320), interpolation=cv2.INTER_CUBIC)\n",
        "        data_upscaled[i] = large_img\n",
        "\n",
        "    return data_upscaled\n",
        "\n",
        "# resize train and  test data\n",
        "x_train_resized = resize_data(x_train)\n",
        "x_test_resized = resize_data(x_test)\n",
        "\n",
        "# make explained variable hot-encoded\n",
        "y_train_hot_encoded = to_categorical(y_train)\n",
        "y_test_hot_encoded = to_categorical(y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jj6ibxf74obH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def resize_data_NASNet(data): #TODO: check \n",
        "    \"\"\"\n",
        "    Resizes data to be compatible with NASNet input requirements\n",
        "\n",
        "    data: image data\n",
        "    \"\"\"\n",
        "    data_upscaled = np.zeros((data.shape[0], 331, 331, 3))\n",
        "    for i, img in enumerate(data):\n",
        "        large_img = cv2.resize(img, dsize=(331, 331), interpolation=cv2.INTER_CUBIC)\n",
        "        data_upscaled[i] = large_img\n",
        "\n",
        "    return data_upscaled\n",
        "\n",
        "# resize train and  test data\n",
        "x_train_resized_NASNet = resize_data_NASNet(x_train)\n",
        "x_test_resized_NASNet = resize_data_NASNet(x_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uag6jdl-vz1u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model(x_train, y_train, base_model, num_epochs: int=10, learning_rate: int=0.0001, min_delta= 0, patience: int=10, batch_size: int=128, val_split: float=0.2):\n",
        "    \"\"\"\n",
        "\n",
        "    x_train: Matrix of image data\n",
        "\n",
        "    y_train: One hot encoded vectors of labels\n",
        "\n",
        "    base_model: Type of pre-trained model\n",
        "\n",
        "    num_epochs (int): Number of passes through train data. Defaulted to 10.\n",
        "\n",
        "    learning_rate (int): Step size of loss function. Defaulted to 0.0001\n",
        "\n",
        "    min_delta: Minimum reduction in val loss for early stopping\n",
        "\n",
        "    patience (int): Number of epochs at min_delta before early stopping. Defaulted to 10.\n",
        "\n",
        "    batch_size (int): Size of mini batches used in training. Defaulted to 128.\n",
        "\n",
        "    val_split (float): Proportion of train data saved for validation. Defaulted to 0.2\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # get layers and add average pooling layer\n",
        "    x = base_model.output\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "\n",
        "    # add fully-connected layer\n",
        "    x = Dense(512, activation='relu')(x)\n",
        "\n",
        "    # add output layer\n",
        "    predictions = Dense(10, activation='softmax')(x)\n",
        "\n",
        "    model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "    # freeze pre-trained model area's layer\n",
        "    for layer in base_model.layers:\n",
        "        layer.trainable = False\n",
        "\n",
        "    # update the weight that are added\n",
        "    model.compile(optimizer='rmsprop', loss='categorical_crossentropy') #TODO: change to SGD?\n",
        "    model.fit(x_train, y_train)\n",
        "\n",
        "    # choose the layers which are updated by training\n",
        "    layer_num = len(model.layers)\n",
        "    for layer in model.layers[:int(layer_num * 0.9)]:\n",
        "        layer.trainable = False\n",
        "\n",
        "    for layer in model.layers[int(layer_num * 0.9):]:\n",
        "        layer.trainable = True\n",
        "    \n",
        "    # update the weights\n",
        "    early_stop = EarlyStopping(monitor='val_loss', min_delta=min_delta, patience=patience, verbose=1, mode='auto')\n",
        "    model.compile(optimizer=SGD(lr=learning_rate, momentum=0.9), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    history = model.fit(x_train, y_train, epochs = num_epochs, batch_size = batch_size, validation_split= val_split, callbacks=[Early_Stop])\n",
        "    train_acc, val_acc, train_loss, val_loss = history.history['acc'], history.history['val_acc'], history.history['loss'], history.history['val_loss']  \n",
        "    train_dict = {\"Train Accuracy\": train_acc, \"Train Loss\": train_loss, \"Val Accuracy\": val_acc, \"Val Loss\": val_loss}\n",
        "    return history, train_dict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c-BOwDgfdXWm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Define hyperparameters\n",
        "MODEL_DICT = {\"InceptionV3\": InceptionV3, \"VGG16\": VGG16, \"DenseNet201\": DenseNet201, \"MobileNetV2\": MobileNetV2, \n",
        "              \"ResNet15V2\": ResNet152V2, \"InceptionResNetV2\": InceptionResNetV2, \"NASNetLarge\": NASNetLarge, \"Xception\": Xception}\n",
        "LEARNING_RATES = [1e-5, 1e-4, 1e-3, 1e-2, 1e-1]\n",
        "MIN_DELTA = 0.0001\n",
        "PATIENCE = 5\n",
        "VAL_SPLIT = 0.2\n",
        "BATCH_SIZES = [64, 128] #Decide on these\n",
        "NUM_EPOCHS = 50\n",
        "OUTPUT_DIR = \"/content/drive/My Drive/Dog_Whistle_Code/Fine_Tuned_Models/Image\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aAn3GJex_Atf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "best_models = {}\n",
        "\n",
        "for model_selection in MODEL_DICT.keys():\n",
        "    print(\"Running {}...\".format(model_selection))\n",
        "\n",
        "    top_f1 = 0\n",
        "\n",
        "    for bs in BATCH_SIZE:\n",
        "        for lr in LEARNING_RATE:\n",
        "            temp_model = MODEL_DICT[model_selection](weights='imagenet', include_top=False)\n",
        "    \n",
        "            if model_selection != \"NASNetLarge\":\n",
        "                history_temp_model, train_dict = model(x_train_resized, y_train_hot_encoded, temp_model, NUM_EPOCHS, lr, MIN_DELTA, PATIENCE, bs, VAL_SPLIT)\n",
        "                #evaluation_temp_model = history_temp_model.model.evaluate(x_test_resized, y_test_hot_encoded)\n",
        "                preds = history_temp_model.model.predict_classes(x_test_resized, verbose=1)\n",
        "                results, predictions, labels = metrics(y_test, preds)\n",
        "            else:\n",
        "                history_temp_model, train_dict = model(x_train_resized_NASNet, y_train_hot_encoded, temp_model,  NUM_EPOCHS, lr, MIN_DELTA, PATIENCE, bs, VAL_SPLIT)\n",
        "                #evaluation_temp_model = history_temp_model.model.evaluate(x_test_resized_NASNet, y_test_hot_encoded)\n",
        "                preds = history_temp_model.model.predict_classes(x_test_resized_NASNet, verbose=1)\n",
        "                results, predictions, labels = metrics(y_test, preds)\n",
        "\n",
        "            if results[\"f1\"] > top_f1:\n",
        "                top_f1 = learning_rate_dict[lr][\"f1\"]\n",
        "                print(\"The new top F1 score is: {}. Saving model...\".format(top_f1))\n",
        "                image_save_model(history_temp_model, model_selection, OUTPUT_DIR , train_dict, y_test, preds, results)\n",
        "                best_models[model_selection] = {\"Learning Rate\": lr, \"Batch_Size\": bs, \"Results\": results} #only save results for best model\n",
        "\n",
        "#save complete training results\n",
        "np.save(os.path.join(OUTPUT_DIR, \"dogwhistle_total_image_training_results.npy\"), results_dict)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g7hyyjPT8cC8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Practice with one iteration\n",
        "temp_model = MobileNetV2(weights='imagenet', include_top=False)\n",
        "history_temp_model = model(x_train_resized, y_train_hot_encoded, temp_model, NUM_EPOCHS, 0.001, MIN_DELTA, PATIENCE, 64, VAL_SPLIT)\n",
        "evaluation_temp_model = history_temp_model.model.evaluate(x_test_resized, y_test_hot_encoded)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}