{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "16nsObF6tLVpLqC_d--F7MaQpfBSF5-EW",
      "authorship_tag": "ABX9TyNp2AFTBfnOgtHlTKDZiAFx"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "tvCdgJ46kFNX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pip install torchtext==0.5.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FrXSHn-HvntM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pip install torchtext"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTv62IihkHkC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pip show torchtext"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kmM1haWiPAn6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pip install import-ipynb"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TKOjnoUdkJ1-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import re\n",
        "import import_ipynb\n",
        "\n",
        "from torch import nn, optim\n",
        "from torchtext.data import Field, Dataset, Example, BucketIterator, Iterator, TabularDataset\n",
        "from tqdm import tqdm, trange\n",
        "\n",
        "%cd \"/content/drive/My Drive/Dog_Whistle_Code\"\n",
        "from HelperFunctions import *"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jYYZk02HCtTH",
        "colab_type": "text"
      },
      "source": [
        "### Prepare Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JfISmDRMCsjK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "NOMARLIZE_LIST = ['url', 'email', 'percent', 'money', 'phone', 'user', 'time', 'date', 'number']\n",
        "ANNOTATE_LIST = ['hashtag', 'allcaps', 'elongated', 'repeated', 'emphasis', 'censored']\n",
        "\n",
        "\n",
        "# Load data\n",
        "train = pd.read_csv(\"\") \n",
        "dev = pd.read_csv(\"\") \n",
        "test = pd.read_csv(\"\") \n",
        "\n",
        "#clean data\n",
        "train[\"text\"] = clean_text(train, NOMARLIZE_LIST, ANNOTATE_LIST)\n",
        "dev[\"text\"] = clean_text(dev, NOMARLIZE_LIST, ANNOTATE_LIST)\n",
        "test[\"text\"] = clean_text(test, NOMARLIZE_LIST, ANNOTATE_LIST)\n",
        "\n",
        "#Subset with necessary data\n",
        "train = train[[\"text\", \"labels\"]]\n",
        "dev = dev[[\"text\", \"labels\"]]\n",
        "test = test[[\"text\", \"labels\"]]\n",
        "\n",
        "# Define fields\n",
        "TEXT = Field(sequential=True, tokenize=\"spacy\", include_lengths=True) \n",
        "LABEL = Field(sequential=False, use_vocab=False)\n",
        "fields = [('text', TEXT), ('labels', LABEL)]\n",
        "\n",
        "# Create datasets\n",
        "train_set = Dataset([Example.fromlist(i, fields) for i in train.values.tolist()], fields=fields)\n",
        "dev_set = Dataset([Example.fromlist(i, fields) for i in dev.values.tolist()], fields=fields)\n",
        "test_set = Dataset([Example.fromlist(i, fields) for i in test.values.tolist()], fields=fields)\n",
        "\n",
        "# Get training words and pretrained vectors (training words without vectors are initialized randomly)\n",
        "TEXT.build_vocab(train_set, vectors='fasttext.simple.300d', unk_init=torch.Tensor.normal_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-23FiNmCve1",
        "colab_type": "text"
      },
      "source": [
        "## Model Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_k9eL9r3kRxE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LSTMClassifier(nn.Module):\n",
        "    def __init__(self, train, dev, test, pad_id, input_dim: int, embedding_dim: int=300, hidden_dim: int=256, output_dim: int=4, n_layers: int=2, dropout: float=0.2, bidirectional: bool=True, batch_size: int=64):\n",
        "        \"\"\"\n",
        "        train: dataset object of training data\n",
        "\n",
        "        dev: dataset object of dev data\n",
        "\n",
        "        test: dataset object of test data\n",
        "        \n",
        "        pad_id: pads output with embedding vector\n",
        "        \n",
        "        input_dim (int): length of text\n",
        "        \n",
        "        embedding_dim (int): number of embedding dimensions defaulted to 300\n",
        "        \n",
        "        hidden_dim (int): number of hidden nodes defaulted to 256\n",
        "        \n",
        "        output_dim (int): number of labels in classification task defaulted to 2\n",
        "        \n",
        "        n_layers (int): number of recurrent layers defaulted to 2\n",
        "        \n",
        "        dropout (float): percent on nodes turned off during training defaulted to 0.2\n",
        "\n",
        "        bidirectional (boolean): whether bidirectional layers will be added, defaulted to True\n",
        "\n",
        "        batch_size (int): size of mini batches\n",
        "        \"\"\"\n",
        "        \n",
        "        super(LSTMClassifier, self).__init__()\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.train_iter, self.dev_iter, self.test_iter = BucketIterator.splits((train, dev, test), \n",
        "                                                                                batch_size=batch_size, \n",
        "                                                                                device=self.device, \n",
        "                                                                                sort_key=lambda x: len(x.text), \n",
        "                                                                                sort_within_batch=True\n",
        "                                                                                )\n",
        "\n",
        "        self.output_dim = output_dim\n",
        "        self.embedding = nn.Embedding(input_dim, embedding_dim, padding_idx=pad_id)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers, bidirectional=bidirectional, dropout=dropout, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
        "        self.sig = nn.Sigmoid()\n",
        "\n",
        "\n",
        "    def forward(self, text, lengths):\n",
        "        \"\"\"\n",
        "        This function sets up the model's foward pass\n",
        "\n",
        "        Initiates forward pass of data\n",
        "        \n",
        "        text: vector representation of text string\n",
        "        \n",
        "        lengths: second element in bucket iterator\n",
        "        \"\"\"\n",
        "\n",
        "        embedded = self.embedding(text)\n",
        "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, lengths) #,batch_first=True)\n",
        "        packed_output, (hidden, cell) = self.lstm(packed_embedded)\n",
        "        hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)\n",
        "        outputs=self.fc(hidden)\n",
        "        \n",
        "        return outputs\n",
        "      \n",
        "    \n",
        "    def trainer(self, model, learning_rate: float, early_stop_vals, epochs: int=50):\n",
        "        \"\"\"\n",
        "        This function trains the model\n",
        "\n",
        "        model: Instantiation of model\n",
        "\n",
        "        learning_rate (float): determines steps size while minimizing loss function\n",
        "        \n",
        "        early_stop_vals: Dictionary containing patience level and minimum improvement delta\n",
        "        \n",
        "        epochs (int): Number of training epochs defaulted to 50\n",
        "        \"\"\"\n",
        "\n",
        "        optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "        model = model.to(self.device)\n",
        "        \n",
        "        train_scores, train_losses, val_scores, val_losses, = [], [], [], []\n",
        "        \n",
        "        for epoch in trange(epochs, desc= \"Epoch\"):\n",
        "            print(\"Processing epoch {}\".format(epoch+1))\n",
        "            if early_stopping(val_losses, early_stop_vals) == False:\n",
        "\n",
        "                model.train()\n",
        "\n",
        "                train_acc = 0\n",
        "                batch_train_losses = []\n",
        "\n",
        "                for step, batch in enumerate(self.train_iter): # Loop over mini-batches\n",
        "\n",
        "                    optimizer.zero_grad()\n",
        "\n",
        "                    text, lengths = batch.text\n",
        "\n",
        "                    if self.output_dim ==  1:\n",
        "                        predictions = model(text, lengths).squeeze()\n",
        "                        criterion = nn.BCELogits().to(self.device)\n",
        "                        loss = criterion(predictions, batch.labels.to(self.device, dtype=torch.float)) \n",
        "                        loss.backward() # Backpropagate loss\n",
        "                        optimizer.step() # Update weights\n",
        "                        train_acc += self.binary_accuracy(predictions, batch.labels).item()\n",
        "                        batch_train_losses.append(loss.tolist())\n",
        "                    else:\n",
        "                        predictions = model(text, lengths)\n",
        "                        criterion = nn.CrossEntropyLoss().to(self.device)\n",
        "                        loss = criterion(predictions, batch.labels) \n",
        "                        loss.backward() # Backpropagate loss\n",
        "                        optimizer.step() # Update weights\n",
        "                        train_acc += self.batch_accuracy(predictions, batch.labels).item() \n",
        "                        batch_train_losses.append(loss.tolist())\n",
        "\n",
        "\n",
        "                batch_loss = sum(batch_train_losses)/len(batch_train_losses)\n",
        "                train_losses.append(batch_loss)\n",
        "                train_acc /= len(self.train_iter)\n",
        "                train_scores.append(train_acc)\n",
        "                train_losses.append()\n",
        "                print('Accuracy on train data:\\t{:.4f}'.format(train_acc))\n",
        "                model.eval() # Compute accuracy on validation data\n",
        "\n",
        "                val_acc = 0\n",
        "                batch_val_losses = []\n",
        "\n",
        "                with torch.no_grad():\n",
        "\n",
        "                    for batch in self.dev_iter:\n",
        "\n",
        "                        text, lengths = batch.text\n",
        "\n",
        "                        if self.output_dim ==  1:\n",
        "                            preds = model(text, lengths).squeeze()\n",
        "                            criterion = nn.BCELogits().to(self.device)\n",
        "                            loss = criterion(preds, batch.labels.to(self.device, dtype=torch.float)) \n",
        "                            batch_val_losses.append(loss.tolist())\n",
        "                            val_acc += self.binary_accuracy(preds, batch.label).item()\n",
        "                        else:\n",
        "                            preds = model(text, lengths)\n",
        "                            criterion = nn.CrossEntropyLoss().to(self.device)\n",
        "                            loss = criterion(preds, batch.labels) \n",
        "                            batch_val_losses.append(loss.tolist())\n",
        "                            val_acc += self.batch_accuracy(preds, batch.labels).item()\n",
        "                        \n",
        "\n",
        "                batch_loss = sum(batch_val_losses)/len(batch_val_losses)\n",
        "                print(\"Batch dev loss: {}\".format(batch_loss))\n",
        "                val_losses.append(batch_loss)\n",
        "                val_acc /= len(self.dev_iter)\n",
        "                val_scores.append(val_acc)\n",
        "\n",
        "                print('Accuracy on dev data:\\t{:.4f}'.format(val_acc))\n",
        "                \n",
        "                if epoch == (epochs-1):\n",
        "                    training_plot(train_losses, val_losses)\n",
        "                    training_dict = {\"Train Accuracy\": train_scores, \"Train Loss\": train_losses, \"Val Accuracy\": val_scores, \"Val Loss\": val_losses}\n",
        "                    print(\"Training complete!\")\n",
        "                    return training_dict\n",
        "                else:\n",
        "                    continue\n",
        "                \n",
        "            else:\n",
        "                print(\"Stopping early...\")\n",
        "                print(\"Training complete!\")\n",
        "                training_plot(train_losses, val_losses)\n",
        "                training_dict = {\"Train Accuracy\": train_scores, \"Train Loss\": train_losses, \"Val Accuracy\": val_scores, \"Val Loss\": val_losses}\n",
        "                print(\"Training complete!\")\n",
        "                return training_dict\n",
        "\n",
        "\n",
        "    def test(self, model):\n",
        "        \"\"\"\n",
        "        This function performs a forward pass on the test data and computes the performance metrics\n",
        "\n",
        "        model: instantiation of model\n",
        "        \"\"\"\n",
        "        \n",
        "        model = model.to(self.device)\n",
        "        model.eval()\n",
        "\n",
        "        labels = list()\n",
        "        preds = list()\n",
        "\n",
        "        with torch.no_grad():\n",
        "\n",
        "            for batch in self.test_iter:\n",
        "\n",
        "                text, lengths = batch.text\n",
        "                output = model(text, lengths)\n",
        "\n",
        "                preds.extend([p.item() for p in output.argmax(dim=1, keepdim=True)])\n",
        "                labels.extend([l.item() for l in batch.labels])\n",
        "        \n",
        "        return metrics(labels, preds)\n",
        "\n",
        "\n",
        "    def batch_accuracy(self, predictions, labels):\n",
        "        \"\"\"\n",
        "        Calculates mean batch accuracy for multiclass data\n",
        "        \n",
        "        predictions: model output after forward pass\n",
        "        \n",
        "        labels: list of one-hot encoded labels\n",
        "        \"\"\"\n",
        "        \n",
        "        max_predictions = predictions.argmax(dim=1, keepdim=True)\n",
        "        correct = max_predictions.squeeze(1).eq(labels)\n",
        "        \n",
        "        return correct.sum() / torch.FloatTensor([labels.shape[0]])\n",
        "\n",
        "\n",
        "    def binary_accuracy(self, predictions, labels):\n",
        "        \"\"\"\n",
        "        Calculates mean batch accuracy for binary data\n",
        "\n",
        "        predictions: model output after forward pass\n",
        "          \n",
        "        labels: list of one-hot encoded labels\n",
        "        \"\"\"\n",
        "        rounded_preds = torch.round(self.sig(predictions))\n",
        "        \n",
        "        correct = (rounded_preds == labels).float() \n",
        "        acc = correct.sum() / len(correct)\n",
        "        \n",
        "        return acc\n",
        "      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iPCHZ6vRC0dN",
        "colab_type": "text"
      },
      "source": [
        "## Run Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WBkt1L2poGUn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#define hyperparameters\n",
        "INPUT_DIM = len(TEXT.vocab)\n",
        "PAD_ID = TEXT.vocab.stoi[TEXT.pad_token]\n",
        "UNK_ID = TEXT.vocab.stoi[TEXT.unk_token]\n",
        "N_EPOCHS = 50\n",
        "EARLY_STOPPING = {\"patience\": 10, \"delta\": 0.01}\n",
        "LEARNING_RATES = [0.0001, 0.001, 0.01, 0.1, 1]\n",
        "OUTPUT_DIR = \"/content/drive/My Drive/Dog_Whistle_Code/Fine_Tuned_Models/Text/LSTM\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8IPIaemaCZRx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "results_dict = {}\n",
        "max_f1_value = 0\n",
        "\n",
        "for i in LEARNING_RATES:\n",
        "    # Initialize LSTM model\n",
        "    model = LSTMClassifier(train_set, dev_set, test_set, PAD_ID, INPUT_DIM)  \n",
        "\n",
        "    # Load pretrained vector\n",
        "    model.embedding.weight.data.copy_(TEXT.vocab.vectors) \n",
        "\n",
        "    # Manually initialize UNK and PAD tokens as zero vectors (and NOT randomly as would be done otherwise)\n",
        "    model.embedding.weight.data[UNK_ID] = torch.zeros(300)\n",
        "    model.embedding.weight.data[PAD_ID] = torch.zeros(300)\n",
        "\n",
        "    #model test\n",
        "    train_dict = model.trainer(model, i, EARLY_STOPPING, N_EPOCHS) \n",
        "    results_dict[i], labels, preds = model.test(model)\n",
        "\n",
        "    if results_dict[i][\"f1\"] >= max_f1_value: #only save best model\n",
        "        max_f1_value = results_dict[i][\"f1\"]\n",
        "        model_saver(model, \"LSTM\", OUTPUT_DIR, train_dict, labels, preds, results_dict[i])\n",
        "\n",
        "#save complete training results\n",
        "np.save(os.path.join(OUTPUT_DIR, \"dogwhistle_total_training_results.npy\"), results_dict)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V6CCoqR1Gr_8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_TICK_LABELS = []\n",
        "Y_TICK_LABELS = []\n",
        "COLOR = \"blues\"\n",
        "SAVE_NAME = \"LSTM_cm_dogwhistle.png\"\n",
        "BEST_RESULTS = \n",
        "\n",
        "confusion_matrix_plotter(BEST_RESULTS, SAVE_NAME, X_TICK_LABELS, Y_TICK_LABELS, COLOR)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}