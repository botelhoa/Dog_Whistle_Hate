{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "tvCdgJ46kFNX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install torchtext==0.5.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FrXSHn-HvntM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install torchtext"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTv62IihkHkC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pip show torchtext"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TKOjnoUdkJ1-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%pylab inline\n",
        "from pandas.io.json import json_normalize\n",
        "import json\n",
        "import torch\n",
        "import re\n",
        "import random\n",
        "import warnings\n",
        "import functools\n",
        "import operator\n",
        "import seaborn as sns\n",
        "\n",
        "from collections import defaultdict, Counter\n",
        "from html import unescape\n",
        "from matplotlib import pyplot as plt\n",
        "from torch import nn, optim\n",
        "from torchtext.data import Field, Dataset, Example, BucketIterator, Iterator, TabularDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import matthews_corrcoef, confusion_matrix, precision_score, recall_score, f1_score, accuracy_score\n",
        "from sklearn.utils import shuffle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jYYZk02HCtTH",
        "colab_type": "text"
      },
      "source": [
        "### Prepare Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "etQnFBhGDHsL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use this code block if load file is only 2 columns: text and labels\n",
        "\n",
        "# Locate data\n",
        "# file_path = \"\"\n",
        "# train_path = \"\"\n",
        "# dev_path = \"\"\n",
        "# test_path = \"\"\n",
        "\n",
        "# # Define fields\n",
        "# TEXT = Field(sequential=True, tokenize=\"spacy\", lower=True, include_lengths=True) \n",
        "# LABEL = Field(sequential=False, use_vocab=False)\n",
        "\n",
        "# # Create dataset object\n",
        "# train_set, dev_set, test_set = TabularDataset.splits(path=file_path, train=train_path,validation=dev_path, test=test_path, format='csv',fields=[('text', TEXT), ('labels', LABEL))\n",
        "\n",
        "# # Get training words and pretrained vectors (training words without vectors are initialized randomly)\n",
        "# TEXT.build_vocab(train_set, vectors='fasttext.simple.300d', unk_init=torch.Tensor.normal_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JfISmDRMCsjK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Remove columns not needed\n",
        "train = pd.read_csv(\"\") \n",
        "dev = pd.read_csv(\"\") \n",
        "test = pd.read_csv(\"\") \n",
        "\n",
        "train = train[[\"text\", \"labels\"]]\n",
        "dev = dev[[\"text\", \"labels\"]]\n",
        "test = test[[\"text\", \"labels\"]]\n",
        "\n",
        "# Define fields\n",
        "TEXT = Field(sequential=True, tokenize=\"spacy\", include_lengths=True) \n",
        "LABEL = Field(sequential=False, use_vocab=False)\n",
        "fields = [('text', TEXT), ('labels', LABEL)]\n",
        "\n",
        "# Create datasets\n",
        "train_set = Dataset([Example.fromlist(i, fields) for i in train.values.tolist()], fields=fields)\n",
        "dev_set = Dataset([Example.fromlist(i, fields) for i in dev.values.tolist()], fields=fields)\n",
        "test_set = Dataset([Example.fromlist(i, fields) for i in test.values.tolist()], fields=fields)\n",
        "\n",
        "# Get training words and pretrained vectors (training words without vectors are initialized randomly)\n",
        "TEXT.build_vocab(train_set, vectors='fasttext.simple.300d', unk_init=torch.Tensor.normal_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-23FiNmCve1",
        "colab_type": "text"
      },
      "source": [
        "## Model Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_k9eL9r3kRxE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LSTMClassifier(nn.Module):\n",
        "    def __init__(self, train, dev, test, pad_id, input_dim: int, embedding_dim: int=300, hidden_dim: int=256, output_dim: int=4, n_layers: int=2, dropout: float=0.2, bidirectional: bool=True, batch_size: int=64):\n",
        "        \"\"\"\n",
        "        train: dataset object of training data\n",
        "\n",
        "        dev: dataset object of dev data\n",
        "\n",
        "        test: dataset object of test data\n",
        "        \n",
        "        pad_id: pads output with embedding vector\n",
        "        \n",
        "        input_dim (int): length of text\n",
        "        \n",
        "        embedding_dim (int): number of embedding dimensions defaulted to 300\n",
        "        \n",
        "        hidden_dim (int): number of hidden nodes defaulted to 256\n",
        "        \n",
        "        output_dim (int): number of labels in classification task defaulted to 2\n",
        "        \n",
        "        n_layers (int): number of recurrent layers defaulted to 2\n",
        "        \n",
        "        dropout (float): percent on nodes turned off during training defaulted to 0.2\n",
        "\n",
        "        bidirectional (boolean): whether bidirectional layers will be added, defaulted to True\n",
        "\n",
        "        batch_size (int): size of mini batches\n",
        "        \"\"\"\n",
        "        \n",
        "        super(LSTMClassifier, self).__init__()\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.train_iter, self.dev_iter, self.test_iter = BucketIterator.splits((train, dev, test), \n",
        "                                                                                batch_size=batch_size, \n",
        "                                                                                device=self.device, \n",
        "                                                                                sort_key=lambda x: len(x.text), \n",
        "                                                                                sort_within_batch=True\n",
        "                                                                                )\n",
        "\n",
        "        self.output_dim = output_dim\n",
        "        self.embedding = nn.Embedding(input_dim, embedding_dim, padding_idx=pad_id)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers, bidirectional=bidirectional, dropout=dropout, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
        "        self.sig = nn.Sigmoid()\n",
        "\n",
        "\n",
        "    def forward(self, text, lengths):\n",
        "        \"\"\"\n",
        "        This function sets up the model's foward pass\n",
        "\n",
        "        Initiates forward pass of data\n",
        "        \n",
        "        text: vector representation of text string\n",
        "        \n",
        "        lengths: second element in bucket iterator\n",
        "        \"\"\"\n",
        "\n",
        "        embedded = self.embedding(text)\n",
        "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, lengths) #,batch_first=True)\n",
        "        packed_output, (hidden, cell) = self.lstm(packed_embedded)\n",
        "        hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)\n",
        "        outputs=self.fc(hidden)\n",
        "        \n",
        "        return outputs\n",
        "      \n",
        "    \n",
        "    def trainer(self, model, learning_rate: float, early_stop_vals, epochs: int=50):\n",
        "        \"\"\"\n",
        "        This function trains the model\n",
        "\n",
        "        model: Instantiation of model\n",
        "\n",
        "        learning_rate (float): determines steps size while minimizing loss function\n",
        "        \n",
        "        early_stop_vals: Dictionary containing patience level and minimum improvement delta\n",
        "        \n",
        "        epochs (int): Number of training epochs defaulted to 50\n",
        "        \"\"\"\n",
        "\n",
        "        self.early_stop_vals = early_stop_vals\n",
        "        optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "        model = model.to(self.device)\n",
        "        \n",
        "        self.val_scores, self.val_losses, = [], []\n",
        "        \n",
        "        for epoch in range(epochs):\n",
        "            print(\"Processing epoch {}\".format(epoch+1))\n",
        "            if self.early_stopping() == False:\n",
        "\n",
        "                model.train()\n",
        "\n",
        "                train_acc = 0\n",
        "\n",
        "                for step, batch in enumerate(self.train_iter): # Loop over mini-batches\n",
        "\n",
        "                    optimizer.zero_grad()\n",
        "\n",
        "                    text, lengths = batch.text\n",
        "\n",
        "                    if self.output_dim ==  1:\n",
        "                        predictions = model(text, lengths).squeeze()\n",
        "                        criterion = nn.BCELogits().to(self.device)\n",
        "                        loss = criterion(predictions, batch.labels.to(self.device, dtype=torch.float)) \n",
        "                        loss.backward() # Backpropagate loss\n",
        "                        optimizer.step() # Update weights\n",
        "                        train_acc += self.binary_accuracy(predictions, batch.labels).item()\n",
        "                    else:\n",
        "                        predictions = model(text, lengths)\n",
        "                        criterion = nn.CrossEntropyLoss().to(self.device)\n",
        "                        loss = criterion(predictions, batch.labels) \n",
        "                        loss.backward() # Backpropagate loss\n",
        "                        optimizer.step() # Update weights\n",
        "                        train_acc += self.batch_accuracy(predictions, batch.labels).item() \n",
        "\n",
        "                train_acc /= len(self.train_iter)\n",
        "                print('Accuracy on train data:\\t{:.4f}'.format(train_acc))\n",
        "                model.eval() # Compute accuracy on validation data\n",
        "\n",
        "                val_acc = 0\n",
        "\n",
        "                with torch.no_grad():\n",
        "\n",
        "                    batch_val_losses = []\n",
        "                    for batch in self.dev_iter:\n",
        "\n",
        "                        text, lengths = batch.text\n",
        "\n",
        "                        if self.output_dim ==  1:\n",
        "                            preds = model(text, lengths).squeeze()\n",
        "                            criterion = nn.BCELogits().to(self.device)\n",
        "                            loss = criterion(preds, batch.labels.to(self.device, dtype=torch.float)) \n",
        "                            batch_val_losses.append(loss.tolist())\n",
        "                            val_acc += self.binary_accuracy(preds, batch.label).item()\n",
        "                        else:\n",
        "                            preds = model(text, lengths)\n",
        "                            criterion = nn.CrossEntropyLoss().to(self.device)\n",
        "                            loss = criterion(preds, batch.labels) \n",
        "                            batch_val_losses.append(loss.tolist())\n",
        "                            val_acc += self.batch_accuracy(preds, batch.labels).item()\n",
        "                        \n",
        "\n",
        "                batch_loss = sum(batch_val_losses)/len(batch_val_losses)\n",
        "                print(\"Batch dev loss: {}\".format(batch_loss))\n",
        "                self.val_losses.append(batch_loss)\n",
        "                val_acc /= len(self.dev_iter)\n",
        "                self.val_scores.append(val_acc)\n",
        "\n",
        "                print('Accuracy on dev data:\\t{:.4f}'.format(val_acc))\n",
        "                \n",
        "                if epoch == (epochs-1):\n",
        "                    print(\"Training complete!\")\n",
        "                    return self.val_scores\n",
        "                else:\n",
        "                    continue\n",
        "                \n",
        "            else:\n",
        "                print(\"Early stop.\")\n",
        "                print(\"Training complete!\")\n",
        "                return self.val_scores\n",
        "    \n",
        "\n",
        "    def early_stopping(self):\n",
        "        \"\"\"\n",
        "        This function adds early stopping capability\n",
        "\n",
        "        Determines whether or not the model will keep running based on the patience and delta given relative to the val loss\n",
        "        \"\"\"\n",
        "        \n",
        "        if len(self.val_losses) > self.early_stop_vals[\"patience\"]:\n",
        "            if self.val_losses[-1] <= np.mean(np.array(self.val_losses[-1-self.early_stop_vals[\"patience\"]:-1])) - self.early_stop_vals[\"delta\"]:\n",
        "                return False\n",
        "            else:\n",
        "                return True\n",
        "    \n",
        "        else:\n",
        "            return False\n",
        "\n",
        "    def test(self, model):\n",
        "        \"\"\"\n",
        "        This function performs a forward pass on the test data and computes the performance metrics\n",
        "\n",
        "        model: instantiation of model\n",
        "        \"\"\"\n",
        "        \n",
        "        model = model.to(self.device)\n",
        "        model.eval()\n",
        "\n",
        "        labels = list()\n",
        "        preds = list()\n",
        "\n",
        "        with torch.no_grad():\n",
        "\n",
        "            for batch in self.test_iter:\n",
        "\n",
        "                text, lengths = batch.text\n",
        "                output = model(text, lengths)\n",
        "\n",
        "                preds.extend([p.item() for p in output.argmax(dim=1, keepdim=True)])\n",
        "                labels.extend([l.item() for l in batch.labels])\n",
        "        \n",
        "        return self.metrics(labels, preds), labels, preds\n",
        "\n",
        "\n",
        "    def batch_accuracy(self, predictions, labels):\n",
        "        \"\"\"\n",
        "        Calculates mean batch accuracy for multiclass data\n",
        "        \n",
        "        predictions: model output after forward pass\n",
        "        \n",
        "        labels: list of one-hot encoded labels\n",
        "        \"\"\"\n",
        "        \n",
        "        max_predictions = predictions.argmax(dim=1, keepdim=True)\n",
        "        correct = max_predictions.squeeze(1).eq(labels)\n",
        "        \n",
        "        return correct.sum() / torch.FloatTensor([labels.shape[0]])\n",
        "\n",
        "\n",
        "    def binary_accuracy(self, predictions, labels):\n",
        "        \"\"\"\n",
        "        Calculates mean batch accuracy for binary data\n",
        "\n",
        "        predictions: model output after forward pass\n",
        "          \n",
        "        labels: list of one-hot encoded labels\n",
        "        \"\"\"\n",
        "        rounded_preds = torch.round(self.sig(predictions))\n",
        "        \n",
        "        correct = (rounded_preds == labels).float() \n",
        "        acc = correct.sum() / len(correct)\n",
        "        \n",
        "        return acc\n",
        "      \n",
        "    \n",
        "    def metrics(self, labels, preds):\n",
        "        \"\"\"\n",
        "        Returns the Matthew's correlation coefficient, accuracy rate, true positive rate, true negative rate, false positive rate, false negative rate, precission, recall, and f1 score\n",
        "\n",
        "        labels: list of correct labels\n",
        "\n",
        "        pred: list of model predictions\n",
        "        \"\"\"\n",
        "\n",
        "        mcc = matthews_corrcoef(labels, preds)\n",
        "        acc = accuracy_score(labels, preds)\n",
        "        cm = confusion_matrix(labels, preds)\n",
        "        precision = precision_score(labels, preds, average= \"weighted\")\n",
        "        recall = recall_score(labels, preds, average= \"weighted\")\n",
        "        f1 = f1_score(labels, preds, average= \"weighted\")\n",
        "\n",
        "        self.results = {\n",
        "            \"mcc\": mcc,\n",
        "            \"acc\": acc,\n",
        "            \"confusion_matrix\": cm,\n",
        "            \"precision\": precision,\n",
        "            \"recall\": recall,\n",
        "            \"f1\": f1\n",
        "        }\n",
        "\n",
        "        return self.results\n",
        "\n",
        "\n",
        "    def save(self, model, output_directory, name, labels, preds):\n",
        "        \"\"\"\n",
        "        This function saves the model to the given directory\n",
        "\n",
        "        model: model to save\n",
        "\n",
        "        output_directory: Folder to save file in\n",
        "\n",
        "        name: name of files\n",
        "\n",
        "        labels: list of ground truth values\n",
        "\n",
        "        preds: list of predictions made by the model\n",
        "        \"\"\"\n",
        "\n",
        "        if not os.path.exists(output_directory):\n",
        "            os.makedirs(output_directory)\n",
        "\n",
        "        file_name = os.path.join(output_directory, name)\n",
        "\n",
        "        torch.save(model.state_dict(), file_name+\"_model\")\n",
        "\n",
        "        training_dict = {\"Val Accuracy\": self.val_scores, \"Val Loss\": self.val_losses}\n",
        "        np.save(file_name+\"_train_results\", training_dict)\n",
        "        np.save(file_name+\"_test_results\", self.results)\n",
        "\n",
        "        test_predictions = pd.DataFrame([labels, preds])\n",
        "        test_predictions = test_predictions.T\n",
        "        test_predictions = test_predictions.rename(columns={0: 'Labels', 1: 'Predictions'})\n",
        "        test_predictions.to_csv(file_name+\"_predictions\")\n",
        "      \n",
        "        return print(\"Saving complete.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iPCHZ6vRC0dN",
        "colab_type": "text"
      },
      "source": [
        "## Run Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WBkt1L2poGUn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#define hyperparameters\n",
        "INPUT_DIM = len(TEXT.vocab)\n",
        "PAD_ID = TEXT.vocab.stoi[TEXT.pad_token]\n",
        "UNK_ID = TEXT.vocab.stoi[TEXT.unk_token]\n",
        "N_EPOCHS = 50\n",
        "EARLY_STOPPING = {\"patience\": 10, \"delta\": 0.01}\n",
        "LEARNING_RATE = 0.001\n",
        "OUTPUT_DIR = \"\"\n",
        "SAVE_NAME = \"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8IPIaemaCZRx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Initialize LSTM model\n",
        "model = LSTMClassifier(train_set, dev_set, test_set, PAD_ID, INPUT_DIM)  \n",
        "\n",
        "# Load pretrained vector\n",
        "model.embedding.weight.data.copy_(TEXT.vocab.vectors) \n",
        "\n",
        "# Manually initialize UNK and PAD tokens as zero vectors (and NOT randomly as would be done otherwise)\n",
        "model.embedding.weight.data[UNK_ID] = torch.zeros(300)\n",
        "model.embedding.weight.data[PAD_ID] = torch.zeros(300)\n",
        "\n",
        "#model test\n",
        "model.trainer(model, LEARNING_RATE, EARLY_STOPPING, N_EPOCHS) \n",
        "\n",
        "#preds, labels = original_data_model.test(original_data_model)\n",
        "results, labels, preds = model.test(model)\n",
        "print(results)\n",
        "model.save(model, OUTPUT_DIR, SAVE_NAME, labels, preds)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V6CCoqR1Gr_8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_cm = pd.DataFrame(results.ravel()[0][\"confusion_matrix\"])\n",
        "\n",
        "fig = plt.figure(figsize=(16, 12))\n",
        " \n",
        "plt.rc('axes', labelsize=14)  \n",
        "plt.rc('xtick', labelsize=12)   \n",
        "plt.rc('ytick', labelsize=12)       \n",
        "\n",
        "plt.subplot(2, 2, 1)\n",
        "g1 = sns.heatmap(df_cm, annot=True, fmt='g', cmap=\"Greys\")\n",
        "g1.set_xlabel('Predicted Label')\n",
        "g1.set_ylabel('True Label', rotation=0) \n",
        "g1.xaxis.set_ticklabels([\"\",\"\", \"\", \"\"], rotation=0) \n",
        "g1.yaxis.set_ticklabels([\"\",\"\", \"\", \"\"], rotation=0) \n",
        "\n",
        "plt.show()\n",
        "#fig.savefig('dogwhistle_lstm_cm.png',bbox_inches='tight')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}