{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "Transformer_Runner.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "67KkJzjicDh8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pip install transformers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m0DNBHSIShzP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pip install import-ipynb"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-ZQ9jttcxdQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import time\n",
        "import random\n",
        "import functools\n",
        "import operator\n",
        "\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from transformers import BertForSequenceClassification, BertTokenizer, RobertaForSequenceClassification, RobertaTokenizer, DistilBertForSequenceClassification, DistilBertTokenizer, AlbertForSequenceClassification, AlbertTokenizer, get_linear_schedule_with_warmup\n",
        "from transformers import BartForSequenceClassification, BartTokenizer, XLNetForSequenceClassification, XLNetTokenizer\n",
        "from sklearn.metrics import matthews_corrcoef, confusion_matrix, accuracy_score, f1_score, precision_score, recall_score\n",
        "from collections import defaultdict, Counter\n",
        "from tqdm import tqdm, trange\n",
        "\n",
        "%cd \"/content/drive/My Drive/Dog_Whistle_Code\"\n",
        "from HelperFunctions import *"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pLA0fzwxcDiX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = pd.read_csv(\"\", encoding='utf-8')\n",
        "dev = pd.read_csv(\"\", encoding='utf-8')\n",
        "test = pd.read_csv(\"\", encoding='utf-8')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CAgcZ2RYcDib",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TransformerClassifier(nn.Module):\n",
        "    def __init__(self, model, num_labels: int, pad_length: int= 64, batch_size: int = 32):\n",
        "        \"\"\"\n",
        "        model: From HuggingFace transformers library\n",
        "\n",
        "        num_labels (int): Number of annotation classes\n",
        "\n",
        "        pad_length (int): Max sentence length. Defaulted to 64.\n",
        "\n",
        "        batch_size (int): Number of sentences in batch. Default is 32.\n",
        "        \"\"\"\n",
        "        super(TransformerClassifier, self).__init__()\n",
        "        self.model_types = {\n",
        "          \"AlBERT\": [AlbertForSequenceClassification, AlbertTokenizer, 'albert-xlarge-v2'],  # 'albert-xlarge-v2' \"albert-large-v2\" 'albert-base-v2' 'albert-xxlarge-v2'\n",
        "          \"BART\": [BartForSequenceClassification, BartTokenizer, \"bart-large\"], \n",
        "          \"BERT\": [BertForSequenceClassification, BertTokenizer, 'bert-large-uncased'], #'bert-base-uncased'\n",
        "          \"DistilBERT\": [DistilBertForSequenceClassification, DistilBertTokenizer, 'distilbert-base-cased'], \n",
        "          \"RoBERTa\": [RobertaForSequenceClassification, RobertaTokenizer, 'roberta-large'], #'roberta-base'\n",
        "          \"XLNet\": [XLNetForSequenceClassification, XLNetTokenizer, \"xlnet-large-cased\"], #\"xlnet-base-cased\"\n",
        "          } \n",
        "        self.model_selection = model\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.model = self.model_types[self.model_selection][0].from_pretrained(self.model_types[self.model_selection][2], num_labels = num_labels).to(self.device)\n",
        "        self.seed_val = 22\n",
        "        self.pad_length = pad_length\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        random.seed(self.seed_val)\n",
        "        np.random.seed(self.seed_val)\n",
        "        torch.manual_seed(self.seed_val)\n",
        "        if self.device == 'cuda':\n",
        "            torch.cuda.manual_seed_all(self.seed_val)\n",
        "\n",
        "    def preprocesser(self, sequence, labels):\n",
        "        \"\"\"\n",
        "        This function converts a string of text into a tokenized format compatible with the selected model\n",
        "\n",
        "        sequence: An iterable series of data (i.e. Pandas Series, list..) where elements are strings\n",
        "\n",
        "        labels: Pandas series containing data annotations\n",
        "        \"\"\"\n",
        "        \n",
        "        self.tokenizer = self.model_types[self.model_selection][1].from_pretrained(self.model_types[self.model_selection][2])\n",
        "\n",
        "        indexed_tokens = []\n",
        "        attention_masks = []\n",
        "\n",
        "\n",
        "        for counter, sentence in enumerate(sequence):\n",
        "            if counter % 1000 == 0:\n",
        "                print(\"Processing row {}\".format(counter))\n",
        "            if counter == len(sequence):\n",
        "                print(\"Done!\")\n",
        "\n",
        "\n",
        "            encoded_dict = self.tokenizer.encode_plus(\n",
        "                      sentence,            \n",
        "                      add_special_tokens = True,\n",
        "                      max_length = self.pad_length,         \n",
        "                      pad_to_max_length = True,\n",
        "                      return_attention_mask = True,  \n",
        "                      return_tensors = 'pt',   \n",
        "                  )\n",
        "  \n",
        "            indexed_tokens.append(encoded_dict['input_ids'])\n",
        "            attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "        return self.batcher(torch.cat(indexed_tokens, dim=0), torch.cat(attention_masks, dim=0), labels)\n",
        "\n",
        "\n",
        "    def batcher(self, indexed_tokens, attention_masks, labels):\n",
        "        \"\"\"\n",
        "        This function creates batches of a specified size to save on memory\n",
        "\n",
        "        indexed_tokens: Tokenized text output by model preprocesser\n",
        "\n",
        "        attention_masks: Attention masks output by model preprocesser\n",
        "\n",
        "        labels: Pandas series containing data annotations\n",
        "        \"\"\"\n",
        "\n",
        "        data = TensorDataset(indexed_tokens, attention_masks, labels)\n",
        "        sampler = SequentialSampler(data)\n",
        "        dataloader = DataLoader(data, sampler=sampler, batch_size = self.batch_size)\n",
        "\n",
        "        return dataloader\n",
        "\n",
        "\n",
        "    def fine_tune(self, train_data, train_labels, dev_data, dev_labels, normalize_list, annotate_list, early_stop_vals: dict, epochs: int = 3, learning_rate: float = 2e-5, weight_decay: float = 0.1, warmup: float = 0.06):   \n",
        "        \"\"\"\n",
        "        Updates pre-trained model's weights based on given dataset\n",
        "\n",
        "        train_data: Pandas series object containing text data for train set\n",
        "\n",
        "        train_labels: Pandas series object containing ground truth annotations for train set\n",
        "\n",
        "        dev_data: Pandas series object containing text data for dev set\n",
        "\n",
        "        dev_labels: Pandas series object containing ground truth annotations for dev set\n",
        "\n",
        "        normalize_list: list of data features to clean\n",
        "\n",
        "        annotate_list: list of data features to annotate\n",
        "\n",
        "        early_stopping: Dictionary containing patience value (int) and delta value (float). The patience determines the number of epochs to wait to achieve the given delta\n",
        "\n",
        "        epochs (int): Number of times to run through all batches. Default value is 3 according to 2-4 recommended in original BERT paper.\n",
        "\n",
        "        learning_rate (float): Default value is 2e-5 according to recommended value from original BERT paper.\n",
        "\n",
        "        weight decay (float): Default value is 0.1 \n",
        "\n",
        "        warmup (float): Default value is 0.06; percentage of training steps in warmup\n",
        "        \"\"\"\n",
        "    \n",
        "        self.early_stop_vals = early_stop_vals\n",
        "        self.train_labels = torch.Tensor(train_labels.values).to(torch.int64)\n",
        "        self.dev_labels = torch.Tensor(dev_labels.values).to(torch.int64)\n",
        "        \n",
        "        clean_train_data = clean_text(train_data, normalize_list, annotate_list)\n",
        "        clean_val_data = clean_text(dev_data, normalize_list, annotate_list)\n",
        "        self.train_dataloader = self.preprocessor(clean_train_data, self.train_labels)\n",
        "        self.val_dataloader = self.preprocessor(clean_val_data, self.dev_labels)\n",
        "\n",
        "        self.optimizer = optim.AdamW(self.model.parameters(), lr = learning_rate, weight_decay = weight_decay)\n",
        "        self.scheduler = get_linear_schedule_with_warmup(self.optimizer, num_warmup_steps = warmup * (len(self.train_dataloader) * epochs), num_training_steps = (1-warmup) * (len(self.train_dataloader) * epochs))\n",
        "\n",
        "        train_loss_values, val_loss_values, train_acc_values, val_acc_values = [], [], [], []\n",
        "\n",
        "        for epoch in trange(epochs, desc= \"Epoch\"):\n",
        "            if early_stopping() == False:\n",
        "                print('======== Epoch {:} / {:} ========'.format(epoch + 1, epochs))\n",
        "                print('Training...')\n",
        "\n",
        "                train_total_loss, train_total_len, train_num_correct = 0, 0, 0\n",
        "\n",
        "                self.model.train()\n",
        "\n",
        "                for step, batch in enumerate(self.train_dataloader):\n",
        "                    if step % 50 == 0:\n",
        "                        print(\"Processing batch...{}\".format(step))\n",
        "                        print(\"  Batch {:>5,}  of  {:>5,}.\".format(step, len(self.train_dataloader)))\n",
        "\n",
        "                    train_total_len += batch[0].shape[0]\n",
        "                    b_input_ids, b_input_mask, b_labels = tuple(t.to(self.device) for t in batch)\n",
        "\n",
        "                    self.model.zero_grad()  \n",
        "\n",
        "                    outputs = self.model(input_ids=b_input_ids, attention_mask=b_input_mask, labels=b_labels.unsqueeze(0))\n",
        "\n",
        "                    train_total_loss += outputs[0].item()\n",
        "                    outputs[0].backward()\n",
        "\n",
        "                    pred = outputs[1].argmax(1, keepdim=True).float()\n",
        "                    correct_tensor = pred.eq(b_labels.float().view_as(pred))\n",
        "                    correct = np.squeeze(correct_tensor.cpu().numpy())\n",
        "                    train_num_correct += np.sum(correct)\n",
        "\n",
        "                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
        "                    self.optimizer.step()\n",
        "                    self.scheduler.step()\n",
        "\n",
        "                train_acc = train_num_correct / train_total_len\n",
        "                train_acc_scores.append(train_acc)\n",
        "                avg_train_loss = train_total_loss / len(self.train_dataloader)            \n",
        "                train_loss_values.append(avg_train_loss)\n",
        "\n",
        "                print()\n",
        "                print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "\n",
        "                print()\n",
        "                print(\"Running Validation...\")\n",
        "                print()\n",
        "\n",
        "                val_total_loss, val_total_len, num_correct = 0, 0, 0 \n",
        "\n",
        "                self.model.eval()\n",
        "\n",
        "                for batch in self.val_dataloader:\n",
        "                    val_total_len += batch[0].shape[0]\n",
        "                    b_input_ids, b_input_mask, b_labels = tuple(t.to(self.device) for t in batch)\n",
        "\n",
        "                    with torch.no_grad():        \n",
        "\n",
        "                        outputs = self.model(b_input_ids, attention_mask=b_input_mask, labels=b_labels.unsqueeze(0)) #Are labels needed?\n",
        "  \n",
        "                    val_total_loss += outputs[0].item()\n",
        "\n",
        "                    pred = outputs[1].argmax(1, keepdim=True).float()\n",
        "                    correct_tensor = pred.eq(b_labels.float().view_as(pred))\n",
        "                    correct = np.squeeze(correct_tensor.cpu().numpy())\n",
        "                    num_correct += np.sum(correct)\n",
        "\n",
        "                val_acc = num_correct / val_total_len\n",
        "                val_acc_scores.append(val_acc)\n",
        "                avg_val_loss = val_total_loss / len(self.val_dataloader)  \n",
        "                val_loss_values.append(avg_val_loss)\n",
        "\n",
        "                print(f\"Epoch | Validation Accuracy | Training Loss | Validation Loss\")\n",
        "                print(f\"{epoch+1:5d} |       {val_acc:.5f}       |    {avg_train_loss:.5f}    |     {avg_val_loss:.5f}\")\n",
        "\n",
        "                print()\n",
        "\n",
        "                if epoch == (epochs-1):\n",
        "                    training_plot(train_loss_values, val_loss_values)\n",
        "                    training_dict = {\"Train Accuracy\": train_acc_scores, \"Train Loss\": train_loss_values, \"Val Accuracy\": val_acc_scores, \"Val Loss\": val_loss_values}\n",
        "                    print(\"Training complete!\")\n",
        "                    return training_dict, self.tokenizer\n",
        "                else:\n",
        "                    continue\n",
        "      \n",
        "            else:\n",
        "                print(\"Stopping early...\")\n",
        "                training_plot(train_loss_values, val_loss_values)\n",
        "                training_dict = {\"Train Accuracy\": train_acc_scores, \"Train Loss\": train_loss_values, \"Val Accuracy\": val_acc_scores, \"Val Loss\": val_loss_values}\n",
        "                print(\"Training complete!\")\n",
        "                return training_dict, self.tokenizer\n",
        "\n",
        "\n",
        "    def test(self, test_data, test_labels, normalize_list, annotate_list):\n",
        "        \"\"\"\n",
        "        Tests the model's performance based on a several metrics\n",
        "\n",
        "        test_data: Pandas series object containing text data\n",
        "\n",
        "        test_labels: Pandas series object containing labels\n",
        "\n",
        "        normalize_list: list of data features to clean\n",
        "\n",
        "        annotate_list: list of data features to annotate\n",
        "        \"\"\"\n",
        "        self.test_labels = torch.Tensor(test_labels.values).to(torch.int64)\n",
        "\n",
        "        clean_test_data = clean_text(test_data, normalize_list, annotate_list)\n",
        "        self.test_dataloader = self.preprocessor(clean_test_data, self.test_labels)\n",
        "    \n",
        "        print('Predicting labels for {} sentences...'.format(len(self.test_labels)))\n",
        "\n",
        "        self.model.eval()\n",
        "\n",
        "        predictions, true_labels = [], []\n",
        "\n",
        "        for batch in self.test_dataloader:\n",
        "            b_input_ids, b_input_mask, b_labels = tuple(t.to(self.device) for t in batch)\n",
        "      \n",
        "            with torch.no_grad():\n",
        "                outputs = self.model(b_input_ids, attention_mask=b_input_mask)\n",
        "\n",
        "            logits = outputs[0].detach().cpu().numpy()\n",
        "            label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "            predictions.append(logits)\n",
        "            true_labels.append(label_ids)\n",
        "\n",
        "        print('    DONE.')\n",
        "\n",
        "        predictions = functools.reduce(operator.iconcat, predictions, [])\n",
        "        true_labels = functools.reduce(operator.iconcat, true_labels, [])\n",
        "    \n",
        "        return metrics(true_labels, predictions, argmax_needed= True)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DOdkQziucDig",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#RoBERTa fine-tuning hyperparameters for GLUE: \n",
        "NOMARLIZE_LIST = ['url', 'email', 'percent', 'money', 'phone', 'user', 'time', 'date', 'number']\n",
        "ANNOTATE_LIST = ['hashtag', 'allcaps', 'elongated', 'repeated', 'emphasis', 'censored']\n",
        "LEARNING_RATE = [1e-5, 2e-5, 3e-5]\n",
        "N_EPOCHS = 10 \n",
        "EARLY_STOPPING = {\"patience\": 2, \"delta\": 0.03}\n",
        "N_LABELS =  4\n",
        "PAD_LENGTH = 64\n",
        "BATCH_SIZE = [16, 32]\n",
        "WEIGHT_DECAY = 0.1 \n",
        "WARMUP = 0.06 \n",
        "OUTPUT_DIR = \"/content/drive/My Drive/Dog_Whistle_Code/Fine_Tuned_Models/Text\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XvPaxxhEcDil",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "results_dict = {}\n",
        "max_f1_value = 0\n",
        "\n",
        "for i in BATCH_SIZE:\n",
        "    learning_rate_dict = {}\n",
        "    for j in LEARNING_RATE: \n",
        "        Classifier = TransformerClassifier(\"DistilBERT\", N_LABELS, PAD_LENGTH, i) \n",
        "        train_dict, tokenizer = Classifier.fine_tune(train[\"text.clean\"], train[\"labels\"], dev[\"text.clean\"], dev[\"labels\"], NORMALIZE_LIST, ANNOTATE_LIST, EARLY_STOPPING, N_EPOCHS, j, WEIGHT_DECAY, WARMUP) \n",
        "        learning_rate_dict[j], labels, preds = Classifier.test(test[\"text.clean\"], test[\"labels\"], NORMALIZE_LIST, ANNOTATE_LIST)\n",
        "\n",
        "    if learning_rate_dict[j][\"f1\"] >= max_f1_value: #only save best model\n",
        "        max_f1_value = learning_rate_dict[j][\"f1\"]\n",
        "        print(\"The new top F1 score is: {}. Saving model...\".format(max_f1_value))\n",
        "        model_saver(Classifier, \"DistilBERT\", OUTPUT_DIR, train_dict, labels, preds, learning_rate_dict[j], tokenizer)\n",
        "\n",
        "    results_dict[i] = learning_rate_dict \n",
        "\n",
        "#save complete training results\n",
        "np.save(os.path.join(os.path.join(OUTPUT_DIR, \"DistlBERT\"), \"dogwhistle_total_training_results.npy\"), results_dict)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "acUrm5B-6CXf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BEST_LR = \n",
        "BEST_BATCH_SIZE = \n",
        "MODEL_LIST = [\"AlBERT\", \"BART\", \"BERT\", \"DistilBERT\", \"RoBERTa\", \"XLNet\"]\n",
        "\n",
        "model_comparison = {}\n",
        "\n",
        "for model in MODEL_LIST:\n",
        "    Classifier = TransformerClassifier(model, N_LABELS, PAD_LENGTH, BEST_BATCH_SIZE) \n",
        "    train_dict, tokenizer = Classifier.fine_tune(train[\"text.clean\"], train[\"labels\"], dev[\"text.clean\"], dev[\"labels\"], NORMALIZE_LIST, ANNOTATE_LIST, EARLY_STOPPING, N_EPOCHS, BEST_LR, WEIGHT_DECAY, WARMUP) \n",
        "    model_comparison[model], labels, preds = Classifier.test(test[\"text.clean\"], test[\"labels\"], NORMALIZE_LIST, ANNOTATE_LIST)\n",
        "    model_saver(Classifier, model, OUTPUT_DIR, train_dict, labels, preds, model_comparison[model], tokenizer)\n",
        "\n",
        "#save complete training results\n",
        "np.save(os.path.join(os.path.join(OUTPUT_DIR, model), \"dogwhistle_total_training_results.npy\"), model_comparison)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L8KesLbvN6IY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_TICK_LABELS = []\n",
        "Y_TICK_LABELS = []\n",
        "COLOR = \"blues\"\n",
        "SAVE_NAME = \"_cm_dogwhistle.png\" #update with best model\n",
        "BEST_RESULTS = \n",
        "\n",
        "confusion_matrix_plotter(BEST_RESULTS, SAVE_NAME, X_TICK_LABELS, Y_TICK_LABELS, COLOR)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}