{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Mutlimodal_Model_two_stage.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "sr5kyLf1Sc-5",
        "Qux_zvLx_oJw"
      ],
      "mount_file_id": "1rfTeoxgnxiDNWiewUXpE8MqZZNgZU8RO",
      "authorship_tag": "ABX9TyOsV7Lz2RUylcS1JO7MZzRs"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "kl7VR9CrSEbU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pip install ekphrasis"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8QCwbjPvSLXC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gByoHXanSNNy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "8c06fd70-8206-4aaa-bc10-4ba423512304"
      },
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import random\n",
        "import functools\n",
        "import operator\n",
        "import cv2\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import TensorDataset, DataLoader, Dataset, SequentialSampler\n",
        "from transformers import get_linear_schedule_with_warmup, RobertaModel, RobertaConfig, RobertaTokenizer, AutoTokenizer, AutoModel, AutoConfig\n",
        "from sklearn.metrics import matthews_corrcoef, confusion_matrix, accuracy_score, f1_score, precision_score, recall_score\n",
        "from tqdm import tqdm, trange\n",
        "# from ekphrasis.classes.preprocessor import TextPreProcessor\n",
        "# from ekphrasis.classes.tokenizer import SocialTokenizer\n",
        "# from ekphrasis.dicts.emoticons import emoticons\n",
        "from keras.models import load_model, Model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lIO_B7UMSQFD",
        "colab_type": "text"
      },
      "source": [
        "### Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cq9v35aHSO4f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean_text(data, normalize_list, annotate_list):\n",
        "        \"\"\"\n",
        "        This function preprocesses the text using the Ekphrasis library\n",
        "        \n",
        "        data: Pandas series object containing strings of text\n",
        "\n",
        "        normalize_list: list of data features to clean\n",
        "\n",
        "        annotate_list: list of data features to annotate\n",
        "        \"\"\"\n",
        "\n",
        "        text_processor = TextPreProcessor(\n",
        "            normalize= normalize_list,\n",
        "            annotate= annotate_list,\n",
        "            fix_html=True,\n",
        "            segmenter=\"twitter\", \n",
        "            unpack_hashtags=True,  \n",
        "            unpack_contractions=True,  \n",
        "            spell_correct_elong=True,  \n",
        "            tokenizer=SocialTokenizer(lowercase=True).tokenize,\n",
        "            dicts=[emoticons]\n",
        "        )\n",
        "\n",
        "        clean_data = data.map(lambda x: \" \".join(text_processor.pre_process_doc(x)))\n",
        "\n",
        "        return clean_data\n",
        "\n",
        "\n",
        "def early_stopping(val_loss_values, early_stop_vals):\n",
        "    \"\"\"\n",
        "    Determines whether or not the model will keep running based on the patience and delta given relative to the val loss\n",
        "    \"\"\"\n",
        "    if len(val_loss_values) > early_stop_vals[\"patience\"]:\n",
        "      if val_loss_values[-1] <= np.mean(np.array(val_loss_values[-1-early_stop_vals[\"patience\"]:-1])) - early_stop_vals[\"delta\"]:\n",
        "        return False\n",
        "      else:\n",
        "        return True\n",
        "    else:\n",
        "      return False\n",
        "\n",
        "\n",
        "def metrics(labels, preds, argmax_needed: bool = False):\n",
        "    \"\"\"\n",
        "    Returns the Matthew's correlation coefficient, accuracy rate, true positive rate, true negative rate, false positive rate, false negative rate, precission, recall, and f1 score\n",
        "    \n",
        "    labels: list of correct labels\n",
        "\n",
        "    pred: list of model predictions\n",
        "\n",
        "    argmax_needed (boolean): converts logits to predictions. Defaulted to false.\n",
        "    \"\"\"\n",
        "\n",
        "    if argmax_needed == True:\n",
        "        preds = np.argmax(preds, axis=1).flatten()\n",
        "\n",
        "    mcc = matthews_corrcoef(labels, preds)\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    cm = confusion_matrix(labels, preds)\n",
        "\n",
        "    f1 = f1_score(labels, preds, average= \"weighted\")\n",
        "    precision = precision_score(labels, preds, average= \"weighted\")\n",
        "    recall = recall_score(labels, preds, average= \"weighted\")\n",
        "\n",
        "    results = {\n",
        "        \"mcc\": mcc,\n",
        "        \"acc\": acc,\n",
        "        \"confusion_matrix\": cm,\n",
        "        \"precision\": precision,\n",
        "        \"recall\": recall,\n",
        "        \"f1\": f1,\n",
        "    }\n",
        "    \n",
        "    return results, labels, preds\n",
        "\n",
        "\n",
        "def combine_text(df):\n",
        "    \"\"\"\n",
        "    Combines tweet and image text into one column\n",
        "\n",
        "    df: Dataframe which holds the data\n",
        "    \"\"\"\n",
        "    combined_text = []\n",
        "\n",
        "    for row_num in range(len(df)):\n",
        "        tweet_text = df.loc[row_num, \"tweet_text\"]\n",
        "        image_text = df.loc[row_num, \"img_text\"]\n",
        "        if type(image_text) == str:\n",
        "            combined_text.append(tweet_text + image_text)\n",
        "        else:\n",
        "            combined_text.append(tweet_text)\n",
        "\n",
        "    return combined_text\n",
        "\n",
        "\n",
        "def training_plot(train_loss_values, val_loss_values):\n",
        "    \"\"\"\n",
        "    Plots loss after each epoch\n",
        "\n",
        "    training_loss_values: list of floats; output from fine_tune function\n",
        "\n",
        "    val_loss_values: list of floats; output from fine_tune function\n",
        "    \"\"\"\n",
        "    sns.set(style='darkgrid')\n",
        "    plt.rcParams[\"figure.figsize\"] = (12,6)\n",
        "\n",
        "    plt.plot(train_loss_values, 'b-o', label=\"train\")\n",
        "    plt.plot(val_loss_values, 'g-o', label=\"valid\")\n",
        "\n",
        "    #plt.title(\"Training and Validation loss\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend()\n",
        "\n",
        "    #plt.savefig(\"dogwhistle_train_plot.png\",bbox_inches='tight')\n",
        "\n",
        "    return plt.show()\n",
        "\n",
        "\n",
        "def model_saver(model, model_type, model_implementation, output_directory, training_dict, labels, preds, ids, results, tokenizer= None):\n",
        "    \"\"\"\n",
        "    Saves Model and other outputs\n",
        "\n",
        "    model: Model to be saved\n",
        "    \n",
        "    model_type (string): Name of model\n",
        "\n",
        "    model_implementation: Keras or Pytorch\n",
        "    \n",
        "    output_directory: Directory to folder to save file in\n",
        "\n",
        "    training_dict: Dictionary of training and validation values \n",
        "\n",
        "    labels: List of labels for test set\n",
        "\n",
        "    preds: List of model predictions after passed through argmax()\n",
        "\n",
        "    results: Dictionary of metrics\n",
        "\n",
        "    tokenizer: Tokenizer to be saved. Defaulted to None.\n",
        "    \"\"\"\n",
        "    \n",
        "    output_directory = os.path.join(output_directory, model_type)\n",
        "    \n",
        "    if not os.path.exists(output_directory):\n",
        "        os.makedirs(output_directory)\n",
        "\n",
        "    os.chdir(output_directory)\n",
        "\n",
        "    np.save(model_type+\"_dogwhistle_train_results.npy\", training_dict) #save training dict\n",
        "    np.save(model_type+\"_dogwhistle_test_results.npy\", results) #save test metrics\n",
        "    \n",
        "    test_predictions = pd.DataFrame([ids, labels, preds]) #save predictions and labels \n",
        "    test_predictions = test_predictions.T\n",
        "    test_predictions = test_predictions.rename(columns={0: 'Ids', 1: 'Labels', 2: 'Predictions'})\n",
        "    test_predictions.to_csv(model_type+\"_dogwhistle_predictions.csv\")\n",
        "\n",
        "    #save models\n",
        "    if model_implementation == \"Pytorch\":\n",
        "        torch.save(model.state_dict(), model_type+\"_model\")\n",
        "\n",
        "    if model_implementation == \"Keras\":\n",
        "        model.save(\"image_model.h5\") #save model\n",
        "\n",
        "    return print(\"Saving complete.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFqtY5wbSZoV",
        "colab_type": "text"
      },
      "source": [
        "### Text Feature Extraction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fomj37N-Saap",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Transformer_features(nn.Module):\n",
        "  def __init__(self, method_type):\n",
        "      \"\"\"\n",
        "      method_type: Extracts features from Bert either using the method in Devlin et al or Sabat el al\n",
        "\n",
        "      \"\"\"\n",
        "      super(Transformer_features, self).__init__()\n",
        "      self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "      if method_type == \"Devlin\":\n",
        "          self.config = AutoConfig.from_pretrained('/content/drive/My Drive/Dog_Whistle_Code/Fine_Tuned_Models/Text/RoBERTa', output_hidden_states = True)\n",
        "          self.model = AutoModel.from_config(self.config).to(self.device)\n",
        "\n",
        "      if method_type == \"Sabat\":\n",
        "          self.model = RobertaModel.from_pretrained('/content/drive/My Drive/Dog_Whistle_Code/Fine_Tuned_Models/Text/RoBERTa').to(self.device)\n",
        "\n",
        "\n",
        "  def forward(self, dataloader, method_type):\n",
        "    \"\"\"\n",
        "    This function recieves tokenized tensors and the sentence pair IDs and returns a sentence embedding for each input sequence\n",
        "\n",
        "    dataloader: dataloader object containing combined text and IDs\n",
        "\n",
        "    method_type: Extracts features from Bert either using the method in Devlin et al or Sabat el al\n",
        "\n",
        "    \"\"\"\n",
        "   \n",
        "    self.model.eval()\n",
        "\n",
        "\n",
        "    if method_type == \"Devlin\": # averages word embeddings to get sentence embeddings, then concatenates last four layers\n",
        "        \n",
        "        combined_layers = torch.zeros(1, 4096).to(self.device)\n",
        "        id_list = []\n",
        "\n",
        "        for batch in dataloader:\n",
        "            with torch.no_grad():\n",
        "                _, _, encoded_layers = self.model(batch[0].to(self.device), attention_mask=batch[1].to(self.device)) #shape [25 x len(tokens) x 100 x 1024]\n",
        "\n",
        "            concat_layers = torch.cat((torch.mean(encoded_layers[-4], dim=1), torch.mean(encoded_layers[-3], dim=1), torch.mean(encoded_layers[-2], dim=1), torch.mean(encoded_layers[-1], dim=1)), dim=1)\n",
        "            combined_layers = torch.cat((combined_layers, concat_layers), dim=0)\n",
        "            id_list.append(batch[2])\n",
        "\n",
        "    if method_type == \"Sabat\": # averages word embeddings from last layer \n",
        "\n",
        "        combined_layers = torch.zeros(1, 1024).to(self.device)\n",
        "        id_list = []\n",
        "\n",
        "        for batch in dataloader:\n",
        "            with torch.no_grad():\n",
        "                output, _ = self.model(batch[0].to(self.device)) #shape [batch_size x pad_length x 1024]\n",
        "\n",
        "            text_features = torch.mean(output, dim=1)\n",
        "            combined_layers = torch.cat((combined_layers, text_features), dim=0)\n",
        "            id_list.append(batch[2])\n",
        "\n",
        "        \n",
        "    combined_layers = combined_layers[1:, :].to(torch.int64) #input len x 4096\n",
        "    id_list = torch.as_tensor(functools.reduce(operator.iconcat, id_list, [])).to(torch.int64) #input length\n",
        "    out_matrix = torch.cat((id_list.unsqueeze(dim= 1).to(self.device), combined_layers.to(self.device)), dim=1)\n",
        "\n",
        "    return out_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Il8QCu_Y7UC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Text Hyperparameters\n",
        "NORMALIZE_LIST = ['url', 'email', 'percent', 'money', 'phone', 'user', 'time', 'date', 'number']\n",
        "ANNOTATE_LIST = ['hashtag', 'allcaps', 'elongated', 'repeated', 'emphasis', 'censored']\n",
        "TOKENIZER = RobertaTokenizer.from_pretrained('/content/drive/My Drive/Dog_Whistle_Code/Fine_Tuned_Models/Text/RoBERTa')\n",
        "PAD_LENGTH = 100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VmGwgDGivTO6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DogWhistleDatasetText(Dataset):\n",
        "    def __init__(self, df, tokenizer, pad_length: int=100):\n",
        "        self.data = df\n",
        "        self.tokenizer = tokenizer\n",
        "        self.pad_length = pad_length\n",
        "         \n",
        "    def __len__(self):\n",
        "        return (self.data.shape[0])\n",
        "    \n",
        "    def __getitem__(self, i):\n",
        "        text = self.data.loc[i, \"combined_text\"] \n",
        "        encoded_dict = self.tokenizer.encode_plus(text, add_special_tokens = True, max_length = self.pad_length, pad_to_max_length = True, return_attention_mask = True, return_tensors = 'pt')\n",
        "\n",
        "        image_number = self.data.loc[i, \"image_number\"]\n",
        "\n",
        "        return (torch.sum(encoded_dict['input_ids'], dim=0), torch.sum(encoded_dict['attention_mask'], dim=0), image_number) #reshape encoded_dict from 1x100 to 100\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24tJblL9WNvs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "88d9a0bb-fad9-43eb-ebcc-37c0e068be06"
      },
      "source": [
        "# Prepare data\n",
        "\n",
        "#Load data\n",
        "train = pd.read_csv(\"/content/drive/My Drive/Dog_Whistle_Code/Data/Train/dog_whistle_train.csv\", encoding='utf-8')\n",
        "dev = pd.read_csv(\"/content/drive/My Drive/Dog_Whistle_Code/Data/Validation/dog_whistle_dev.csv\", encoding='utf-8')\n",
        "test = pd.read_csv(\"/content/drive/My Drive/Dog_Whistle_Code/Data/Test/dog_whistle_test.csv\", encoding='utf-8')\n",
        "\n",
        "\n",
        "#Clean data\n",
        "train[\"combined_text\"] = combine_text(train)\n",
        "train[\"combined_text\"] = clean_text(train[\"combined_text\"], NORMALIZE_LIST, ANNOTATE_LIST)\n",
        "dev[\"combined_text\"] = combine_text(dev)\n",
        "dev[\"combined_text\"] = clean_text(dev[\"combined_text\"], NORMALIZE_LIST, ANNOTATE_LIST)\n",
        "test[\"combined_text\"] = combine_text(test)\n",
        "test[\"combined_text\"] = clean_text(test[\"combined_text\"], NORMALIZE_LIST, ANNOTATE_LIST)\n",
        "\n",
        "\n",
        "#Subset necessary data\n",
        "train = train[[\"image_number\", \"combined_text\"]]\n",
        "dev = dev[[\"image_number\", \"combined_text\"]] \n",
        "test = test[[\"image_number\", \"combined_text\"]] \n",
        "\n",
        "\n",
        "#Create Dataset\n",
        "train_dataset = DogWhistleDatasetText(train, TOKENIZER)\n",
        "dev_dataset = DogWhistleDatasetText(dev, TOKENIZER)\n",
        "test_dataset = DogWhistleDatasetText(test, TOKENIZER)\n",
        "\n",
        "\n",
        "#Create dataloader\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=32)\n",
        "dev_dataloader = DataLoader(dev_dataset, batch_size=32) \n",
        "test_dataloader = DataLoader(test_dataset, batch_size=32)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading twitter - 1grams ...\n",
            "Reading twitter - 2grams ...\n",
            "Reading english - 1grams ...\n",
            "Reading twitter - 1grams ...\n",
            "Reading twitter - 2grams ...\n",
            "Reading english - 1grams ...\n",
            "Reading twitter - 1grams ...\n",
            "Reading twitter - 2grams ...\n",
            "Reading english - 1grams ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LiflRqFSXZbr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "f9a537df-829b-4e9f-f371-b8be9e2fd2f0"
      },
      "source": [
        "TextExtractor = Transformer_features(\"Devlin\")\n",
        "\n",
        "train_text_features = TextExtractor(train_dataloader, \"Devlin\")\n",
        "print(\"Done\")\n",
        "dev_text_features = TextExtractor(dev_dataloader, \"Devlin\")\n",
        "print(\"Done\")\n",
        "test_text_features = TextExtractor(test_dataloader, \"Devlin\")\n",
        "print(\"Done\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Done\n",
            "Done\n",
            "Done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gUyNUXb1xVt-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "db2c9a56-5ce5-4ae0-9c8a-737eddfa8bd1"
      },
      "source": [
        "TextExtractor = Transformer_features(\"Sabat\")\n",
        "\n",
        "train_text_features = TextExtractor(train_dataloader, \"Sabat\")\n",
        "print(\"Done\")\n",
        "dev_text_features = TextExtractor(dev_dataloader, \"Sabat\")\n",
        "print(\"Done\")\n",
        "test_text_features = TextExtractor(test_dataloader, \"Sabat\")\n",
        "print(\"Done\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Done\n",
            "Done\n",
            "Done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sfLMwmRfXZoK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Save Devlin\n",
        "train_text_features = train_text_features.cpu().numpy()\n",
        "np.save(\"/content/drive/My Drive/Dog_Whistle_Code/Data/Train/text_features.npy\", train_text_features)\n",
        "\n",
        "dev_text_features = dev_text_features.cpu().numpy()\n",
        "np.save(\"/content/drive/My Drive/Dog_Whistle_Code/Data/Validation/text_features.npy\", dev_text_features)\n",
        "\n",
        "test_text_features = test_text_features.cpu().numpy()\n",
        "np.save(\"/content/drive/My Drive/Dog_Whistle_Code/Data/Test/text_features.npy\", test_text_features)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dHieuHuXxbXw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Save Sabat\n",
        "train_text_features= train_text_features.cpu().numpy()\n",
        "np.save(\"/content/drive/My Drive/Dog_Whistle_Code/Data/Train/text_features_sabat.npy\", train_text_features)\n",
        "\n",
        "dev_text_features = dev_text_features.cpu().numpy()\n",
        "np.save(\"/content/drive/My Drive/Dog_Whistle_Code/Data/Validation/text_features_sabat.npy\", dev_text_features)\n",
        "\n",
        "test_text_features = test_text_features.cpu().numpy()\n",
        "np.save(\"/content/drive/My Drive/Dog_Whistle_Code/Data/Test/text_features_sabat.npy\", test_text_features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sr5kyLf1Sc-5",
        "colab_type": "text"
      },
      "source": [
        "### Image Feature Extraction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VAvbwHFG9lCV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Image_features(trained_model, dataloader):\n",
        "  \"\"\" Extracts image features from images\n",
        "\n",
        "  trained_model: pre-trained image model\n",
        "\n",
        "  dataloader: dataloader object containing image paths and IDs\n",
        "  \"\"\"\n",
        "\n",
        "  combined_output = np.zeros((1, 1024))\n",
        "  id_list = []\n",
        "  \n",
        "  for num, batch in enumerate(dataloader):\n",
        "      if num % 25 == 0:\n",
        "          print(\"Processing batch {} of {}\".format(num, len(dataloader)))\n",
        "      batch_output = Model(trained_model.input, trained_model.layers[-2].output).predict(batch[0]) #32 x 1024\n",
        "      combined_output = np.concatenate((combined_output, batch_output), axis=0)\n",
        "      id_list.append(batch[1])\n",
        "\n",
        "\n",
        "  combined_output = combined_output[1:, :]\n",
        "  id_list = np.array(functools.reduce(operator.iconcat, id_list, []))\n",
        "                            \n",
        "  out_matrix = np.concatenate((np.expand_dims(id_list, axis=1), combined_output), axis=1)\n",
        "\n",
        "  return out_matrix\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JNZct58QdBcg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DogWhistleDatasetImage(Dataset):\n",
        "    def __init__(self, df, base_path, image_size: int=299):\n",
        "        self.data = df\n",
        "        self.base_path = base_path\n",
        "        self.image_size = image_size\n",
        "\n",
        "    def __len__(self):\n",
        "        return (self.data.shape[0])\n",
        "    \n",
        "    def __getitem__(self, i):\n",
        "        image_path = str(self.data.loc[i, \"image_number\"])\n",
        "        path = self.base_path + \"/\" + image_path + \".jpg\"\n",
        "        image = cv2.imread(path) \n",
        "        image = cv2.resize(image, (self.image_size, self.image_size)) \n",
        "\n",
        "        sample = (image, self.data.loc[i, \"image_number\"])\n",
        "\n",
        "        return sample"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nqoghT6ndKTW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Load data\n",
        "train = pd.read_csv(\"/content/drive/My Drive/Dog_Whistle_Code/Data/Train/dog_whistle_train.csv\", encoding='utf-8')\n",
        "dev = pd.read_csv(\"/content/drive/My Drive/Dog_Whistle_Code/Data/Validation/dog_whistle_dev.csv\", encoding='utf-8')\n",
        "test = pd.read_csv(\"/content/drive/My Drive/Dog_Whistle_Code/Data/Test/dog_whistle_test.csv\", encoding='utf-8')\n",
        "\n",
        "\n",
        "#Subset necessary data\n",
        "train = train[[\"image_number\"]]\n",
        "dev = dev[[\"image_number\"]] \n",
        "test = test[[\"image_number\"]] \n",
        "\n",
        "\n",
        "#Create Dataset\n",
        "train_dataset = DogWhistleDatasetImage(train, \"/content/drive/My Drive/Dog_Whistle_Code/Data/Images\")\n",
        "dev_dataset = DogWhistleDatasetImage(dev, \"/content/drive/My Drive/Dog_Whistle_Code/Data/Images\")\n",
        "test_dataset = DogWhistleDatasetImage(test, \"/content/drive/My Drive/Dog_Whistle_Code/Data/Images\")\n",
        "\n",
        "\n",
        "#Create dataloader\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=32)\n",
        "dev_dataloader = DataLoader(dev_dataset, batch_size=32) \n",
        "test_dataloader = DataLoader(test_dataset, batch_size=32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QaMcoYnOSfGV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "004d1c89-4a6e-445e-b83a-c975fdd0624b"
      },
      "source": [
        "ImageExtractor = load_model('/content/drive/My Drive/Dog_Whistle_Code/Fine_Tuned_Models/Image/Xception/image_model.h5') #using pre-trained Xception\n",
        "\n",
        "train_image_features = Image_features(ImageExtractor, train_dataloader)\n",
        "dev_image_features = Image_features(ImageExtractor, dev_dataloader)\n",
        "test_image_features = Image_features(ImageExtractor, test_dataloader)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processing batch 0 of 125\n",
            "Processing batch 25 of 125\n",
            "Processing batch 50 of 125\n",
            "Processing batch 75 of 125\n",
            "Processing batch 100 of 125\n",
            "Processing batch 0 of 16\n",
            "Processing batch 0 of 16\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5DB5NwDa8jJw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Save\n",
        "np.save(\"/content/drive/My Drive/Dog_Whistle_Code/Data/Train/image_features.npy\", train_image_features)\n",
        "np.save(\"/content/drive/My Drive/Dog_Whistle_Code/Data/Validation/image_features.npy\", dev_image_features)\n",
        "np.save(\"/content/drive/My Drive/Dog_Whistle_Code/Data/Test/image_features.npy\", test_image_features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qux_zvLx_oJw",
        "colab_type": "text"
      },
      "source": [
        "### Combine Feature Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HzkfkC-g_ocR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load Text data\n",
        "# train_text = np.load(\"/content/drive/My Drive/Dog_Whistle_Code/Data/Train/text_features.npy\", allow_pickle=True)\n",
        "# dev_text = np.load(\"/content/drive/My Drive/Dog_Whistle_Code/Data/Validation/text_features.npy\", allow_pickle=True)\n",
        "# test_text = np.load(\"/content/drive/My Drive/Dog_Whistle_Code/Data/Test/text_features.npy\", allow_pickle=True)\n",
        "train_text = np.load(\"/content/drive/My Drive/Dog_Whistle_Code/Data/Train/text_features_sabat.npy\", allow_pickle=True)\n",
        "dev_text = np.load(\"/content/drive/My Drive/Dog_Whistle_Code/Data/Validation/text_features_sabat.npy\", allow_pickle=True)\n",
        "test_text = np.load(\"/content/drive/My Drive/Dog_Whistle_Code/Data/Test/text_features_sabat.npy\", allow_pickle=True)\n",
        "\n",
        "\n",
        "# Load Image data\n",
        "train_image = np.load(\"/content/drive/My Drive/Dog_Whistle_Code/Data/Train/image_features.npy\", allow_pickle=True)\n",
        "dev_image = np.load(\"/content/drive/My Drive/Dog_Whistle_Code/Data/Validation/image_features.npy\", allow_pickle=True)\n",
        "test_image = np.load(\"/content/drive/My Drive/Dog_Whistle_Code/Data/Test/image_features.npy\", allow_pickle=True)\n",
        "\n",
        "# Load Other data\n",
        "train2 = pd.read_csv(\"/content/drive/My Drive/Dog_Whistle_Code/Data/Train/dog_whistle_train.csv\", encoding='utf-8')\n",
        "dev2 = pd.read_csv(\"/content/drive/My Drive/Dog_Whistle_Code/Data/Validation/dog_whistle_dev.csv\", encoding='utf-8')\n",
        "test2 = pd.read_csv(\"/content/drive/My Drive/Dog_Whistle_Code/Data/Test/dog_whistle_test.csv\", encoding='utf-8')\n",
        "\n",
        "# Merge\n",
        "train = pd.concat((pd.DataFrame(train_text[:, 1:]), pd.DataFrame(train_image[:, 1:])), axis = 1)\n",
        "train[\"ids\"] = train_text[:, :1]\n",
        "train[\"labels\"] = train2[\"Primary_numeric_gt\"]\n",
        "dev = pd.concat((pd.DataFrame(dev_text[:, 1:]), pd.DataFrame(dev_image[:, 1:])), axis = 1)\n",
        "dev[\"ids\"] = dev_text[:, :1]\n",
        "dev[\"labels\"] = dev2[\"Primary_numeric_gt\"]\n",
        "test = pd.concat((pd.DataFrame(test_text[:, 1:]), pd.DataFrame(test_image[:, 1:])), axis = 1)\n",
        "test[\"ids\"] = test_text[:, :1]\n",
        "test[\"labels\"] = test2[\"Primary_numeric_gt\"]\n",
        "\n",
        "# Save\n",
        "# train.to_csv(\"/content/drive/My Drive/Dog_Whistle_Code/Data/Train/combined_features.csv\")\n",
        "# dev.to_csv(\"/content/drive/My Drive/Dog_Whistle_Code/Data/Validation/combined_features.csv\")\n",
        "# test.to_csv(\"/content/drive/My Drive/Dog_Whistle_Code/Data/Test/combined_features.csv\")\n",
        "train.to_csv(\"/content/drive/My Drive/Dog_Whistle_Code/Data/Train/combined_features_sabat.csv\")\n",
        "dev.to_csv(\"/content/drive/My Drive/Dog_Whistle_Code/Data/Validation/combined_features_sabat.csv\")\n",
        "test.to_csv(\"/content/drive/My Drive/Dog_Whistle_Code/Data/Test/combined_features_sabat.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7TwV6IhzWSGF",
        "colab_type": "text"
      },
      "source": [
        "### Pytorch Implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PFrtF544WeoU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MultimodalClassifier(nn.Module):\n",
        "    def __init__(self, MLP_type, hidden_size: int=50, dropout: float=0.2, num_labels: int=4, input_len: int = 5120):\n",
        "        \"\"\"Initializes the network structure\n",
        "        MLP_type: Which paper's MLP structure to use\n",
        "        \n",
        "        image_model: CovNet from Keras library to use as image feature extractor\n",
        "\n",
        "        text_model: Transformer Model from Hugging Face to use as the text feature extractor\n",
        "\n",
        "        hidden_size (int): Number of nodes in the hidden layer. Defaulted to 50. \n",
        "\n",
        "        dropout (float): Rate at which nodes are deactivated. Defaulted to 0.2. \n",
        "        \n",
        "        num_labels (int): Number of labels to predict. Defaulted to 4.\n",
        "\n",
        "        input_len (int): Length of input vector. Defaulted to 5120 (Image feature length (4096) + text feature length (1024)).\n",
        "        \"\"\"\n",
        "        super(MultimodalClassifier, self).__init__()\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "        if MLP_type == \"Sabat\":\n",
        "            self.classifier = nn.Sequential(\n",
        "                              nn.Linear(input_len, hidden_size),\n",
        "                              nn.ReLU(),\n",
        "                              nn.Dropout(dropout),\n",
        "                              nn.Linear(hidden_size, hidden_size),\n",
        "                              nn.ReLU(),\n",
        "                              #nn.Dropout(dropout),\n",
        "                              nn.Linear(hidden_size, num_labels)\n",
        "                              #nn.Softmax(dim=1) \n",
        "                          )\n",
        "\n",
        "        if MLP_type == \"Gomez\":\n",
        "            self.classifier = nn.Sequential(\n",
        "                              nn.Linear(input_len, input_len),\n",
        "                              nn.BatchNorm1d(input_len),\n",
        "                              nn.ReLU(),\n",
        "                              #nn.Dropout(dropout),\n",
        "                              nn.Linear(input_len, 1024),\n",
        "                              nn.BatchNorm1d(1024),\n",
        "                              nn.ReLU(),\n",
        "                              #nn.Dropout(dropout),\n",
        "                              nn.Linear(1024, 512),\n",
        "                              nn.BatchNorm1d(512),\n",
        "                              nn.ReLU(),\n",
        "                              #nn.Dropout(dropout),\n",
        "                              nn.Linear(512, num_labels),\n",
        "                              nn.Softmax(dim=1) \n",
        "                          )\n",
        "\n",
        "\n",
        "    def forward(self, features):\n",
        "        \"\"\"Initiaties foward pass through network\n",
        "        \n",
        "        features: Matrix of size number of tweets x 5120 containing concatenated image and text features\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        out = self.classifier(features.to(torch.float))\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "    def trainer(self, input_model, train_data, dev_data, early_stop_vals: dict, epochs: int = 25, learning_rate: float = 1e-5, weight_decay: float = 0.1, warmup: float = 0.06):   \n",
        "            \"\"\"\n",
        "            Trains multimodal model\n",
        "\n",
        "            input_model: Instatiation of model\n",
        "\n",
        "            train_data: Dataloader object containing train data- image, text, labels\n",
        "\n",
        "            dev_data: Dataloader object containing dev data- image, text, labels\n",
        "\n",
        "            early_stopping: Dictionary containing patience value (int) and delta value (float). The patience determines the number of epochs to wait to achieve the given delta\n",
        "\n",
        "            epochs (int): Number of times to run through all batches. Default value is 25.\n",
        "\n",
        "            learning_rate (float): Default value is 1e-5.\n",
        "\n",
        "            weight decay (float): Default value is 0.1 \n",
        "\n",
        "            warmup (float): Default value is 0.06; percentage of training steps in warmup.\n",
        "            \"\"\"\n",
        "\n",
        "            model = input_model.to(self.device)\n",
        "            self.optimizer = optim.AdamW(model.classifier.parameters(), lr = learning_rate, weight_decay = weight_decay)\n",
        "            self.scheduler = get_linear_schedule_with_warmup(self.optimizer, num_warmup_steps = warmup * (len(train_data) * epochs), num_training_steps = (1-warmup) * (len(train_data) * epochs))\n",
        "            criterion = nn.CrossEntropyLoss().to(self.device)\n",
        "\n",
        "            train_loss_values, val_loss_values, train_acc_values, val_acc_values = [], [], [], []\n",
        "\n",
        "            for epoch in trange(epochs, desc= \"Epoch\"):\n",
        "                if early_stopping(val_loss_values, early_stop_vals) == False:\n",
        "                    print('======== Epoch {:} / {:} ========'.format(epoch + 1, epochs))\n",
        "                    print('Training...')\n",
        "\n",
        "                    train_total_loss, train_total_len, train_num_correct = 0, 0, 0\n",
        "\n",
        "                    model.train()\n",
        "\n",
        "                    for step, batch in enumerate(train_data): \n",
        "                        if step % 50 == 0:\n",
        "                            print(\"Processing batch...{} of {}\".format(step, len(train_data)))\n",
        "\n",
        "                        #model.zero_grad()\n",
        "                        self.optimizer.zero_grad()  \n",
        "                        batch_features, batch_labels, _ = tuple(t.to(self.device) for t in batch)\n",
        "                        train_total_len += batch_features.shape[0]\n",
        "\n",
        "                        logits = model(batch_features)\n",
        "\n",
        "                        loss = criterion(logits, batch_labels).to(self.device) \n",
        "                        train_total_loss += loss\n",
        "\n",
        "                        loss.backward() \n",
        "                        self.optimizer.step() \n",
        "                        self.scheduler.step()\n",
        "\n",
        "                        pred = logits.argmax(1, keepdim=True).float()\n",
        "                        correct_tensor = pred.eq(batch_labels.float().view_as(pred))\n",
        "                        correct = np.squeeze(correct_tensor.cpu().numpy())\n",
        "                        train_num_correct += np.sum(correct)\n",
        "\n",
        "                        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "\n",
        "                    train_acc = train_num_correct / train_total_len\n",
        "                    train_acc_values.append(train_acc)\n",
        "                    avg_train_loss = train_total_loss / len(train_data)         \n",
        "                    train_loss_values.append(avg_train_loss)\n",
        "\n",
        "                    print()\n",
        "                    print(\"Running Validation...\")\n",
        "                    print()\n",
        "\n",
        "                    val_total_loss, val_total_len, val_num_correct = 0, 0, 0 \n",
        "\n",
        "                    model.eval()\n",
        "\n",
        "                    for batch in dev_data:\n",
        "                        batch_features, batch_labels, _ = tuple(t.to(self.device) for t in batch)\n",
        "                        val_total_len += batch_features.shape[0]\n",
        "\n",
        "                        with torch.no_grad():        \n",
        "\n",
        "                            logits = model(batch_features)\n",
        "      \n",
        "                        loss = criterion(logits, batch_labels) \n",
        "                        val_total_loss += loss\n",
        "                      \n",
        "                        pred = logits.argmax(1, keepdim=True).float()\n",
        "                        correct_tensor = pred.eq(batch_labels.float().view_as(pred))\n",
        "                        correct = np.squeeze(correct_tensor.cpu().numpy())\n",
        "                        val_num_correct += np.sum(correct)\n",
        "\n",
        "                    val_acc = val_num_correct / val_total_len\n",
        "                    val_acc_values.append(val_acc)\n",
        "                    avg_val_loss = val_total_loss / len(dev_data)\n",
        "                    val_loss_values.append(avg_val_loss.cpu().numpy())\n",
        "\n",
        "                    print(\"Epoch | Train Accuracy | Validation Accuracy | Training Loss | Validation Loss\")\n",
        "                    print(f\"{epoch+1:3d} |    {train_acc:.3f}    |       {val_acc:.3f}       |    {avg_train_loss:.3f}    |     {avg_val_loss:.3f}\")\n",
        "                    print()\n",
        "\n",
        "\n",
        "                    if epoch == (epochs-1):\n",
        "                        training_plot(train_loss_values, val_loss_values)\n",
        "                        training_dict = {\"Train Accuracy\": train_acc_values, \"Train Loss\": train_loss_values, \"Val Accuracy\": val_acc_values, \"Val Loss\": val_loss_values}\n",
        "                        print(\"Training complete!\")\n",
        "                        return training_dict\n",
        "                    else:\n",
        "                        continue\n",
        "          \n",
        "                else:\n",
        "                    print(\"Stopping early...\")\n",
        "                    training_plot(train_loss_values, val_loss_values)\n",
        "                    training_dict = {\"Train Accuracy\": train_acc_values, \"Train Loss\": train_loss_values, \"Val Accuracy\": val_acc_valuess, \"Val Loss\": val_loss_values}\n",
        "                    print(\"Training complete!\")\n",
        "                    return training_dict\n",
        "\n",
        "\n",
        "    def test(self, input_model, test_data):\n",
        "        \"\"\"\n",
        "        Tests the model's performance based on a several metrics\n",
        "\n",
        "        input_model: Instatiation of model\n",
        "\n",
        "        test_data: Dataloader object containing test data- image, text, labels\n",
        "        \"\"\"\n",
        "   \n",
        "        print('Predicting labels for {} sentences...'.format(len(test_data)))\n",
        "        \n",
        "        model = input_model.to(self.device)\n",
        "        model.eval()\n",
        "\n",
        "        predictions, true_labels, ids = [], [], []\n",
        "\n",
        "        for batch in test_data:\n",
        "            batch_features, batch_labels, batch_ids = tuple(t.to(self.device) for t in batch)\n",
        "      \n",
        "            with torch.no_grad():\n",
        "                logits = model(batch_features)\n",
        "\n",
        "            predictions.append(logits.detach().cpu().numpy())\n",
        "            true_labels.append(batch_labels.to('cpu').numpy())\n",
        "            ids.append(batch_ids.cpu().numpy())\n",
        "\n",
        "        predictions = functools.reduce(operator.iconcat, predictions, [])\n",
        "        true_labels = functools.reduce(operator.iconcat, true_labels, [])\n",
        "        ids = functools.reduce(operator.iconcat, ids, [])\n",
        "\n",
        "        print('    DONE.')\n",
        "    \n",
        "        return metrics(true_labels, predictions, argmax_needed= True), ids\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JfEFzoVCeHqw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Hyperparamters\n",
        "DROPOUT = 0.2\n",
        "HIDDEN_SIZE = 100\n",
        "BATCH_SIZE = 8\n",
        "NUM_LABELS = 4\n",
        "NUM_EPOCHS = 100\n",
        "EARLY_STOPPING = {\"patience\": 5, \"delta\": 0.005}\n",
        "LEARNING_RATES = [0.0001, 0.001, 0.01, 0.1]\n",
        "WEIGHT_DECAY = 0.1 \n",
        "WARMUP = 0.06 \n",
        "OUTPUT_DIR = \"/content/drive/My Drive/Dog_Whistle_Code/Fine_Tuned_Models/Multimodal/Feature Concatenation\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jALfVCXYXkcs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DogWhistleDataset(Dataset):\n",
        "    def __init__(self, df):\n",
        "        self.data = df\n",
        "\n",
        "    def __len__(self):\n",
        "        return (self.data.shape[0])\n",
        "    \n",
        "    def __getitem__(self, i):\n",
        "        features = np.array(self.data.iloc[0, 1:-2]) #start at 1 because of the Unnamed:0 header that gets added\n",
        "        labels = self.data.loc[i, \"labels\"]\n",
        "        ids = self.data.loc[i, \"ids\"]\n",
        "        sample = (features, labels, ids)\n",
        "\n",
        "        return sample"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7VrWKhGwezK7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load data\n",
        "# train = pd.read_csv(\"/content/drive/My Drive/Dog_Whistle_Code/Data/Train/combined_features.csv\")\n",
        "# dev = pd.read_csv(\"/content/drive/My Drive/Dog_Whistle_Code/Data/Validation/combined_features.csv\")\n",
        "# test = pd.read_csv(\"/content/drive/My Drive/Dog_Whistle_Code/Data/Test/combined_features.csv\")\n",
        "\n",
        "train = pd.read_csv(\"/content/drive/My Drive/Dog_Whistle_Code/Data/Train/combined_features_sabat.csv\")\n",
        "dev = pd.read_csv(\"/content/drive/My Drive/Dog_Whistle_Code/Data/Validation/combined_features_sabat.csv\")\n",
        "test = pd.read_csv(\"/content/drive/My Drive/Dog_Whistle_Code/Data/Test/combined_features_sabat.csv\")\n",
        "\n",
        "# Create dataset object\n",
        "train_dataset = DogWhistleDataset(train)\n",
        "dev_dataset = DogWhistleDataset(dev)\n",
        "test_dataset = DogWhistleDataset(test)\n",
        "\n",
        "# Create dataloader\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "dev_dataloader = DataLoader(dev_dataset, batch_size=BATCH_SIZE, shuffle=True) \n",
        "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aN72he8NgPPv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Classifier = MultimodalClassifier(\"Sabat\", HIDDEN_SIZE, DROPOUT, NUM_LABELS, 2048)\n",
        "#logits, batch_labels = Classifier.trainer(Classifier, train_dataloader, dev_dataloader, EARLY_STOPPING, 10, 0.1, WEIGHT_DECAY, WARMUP)\n",
        "train_dict = Classifier.trainer(Classifier, train_dataloader, dev_dataloader, EARLY_STOPPING, 5, LEARNING_RATES[0], WEIGHT_DECAY, WARMUP)\n",
        "(metric_vals, labels, preds), ids = Classifier.test(Classifier, test_dataloader)\n",
        "#model_saver(Classifier, \"Multimodal\", OUTPUT_DIR, train_dict, labels, preds, metrics, ids)\n",
        "\n",
        "print(metric_vals)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-5e1TQpSUjo",
        "colab_type": "text"
      },
      "source": [
        "### Random Forrest "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41y0OhfRmMbD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "df4cf554-12ee-41a2-bc89-0bb96f4b7228"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "sc = StandardScaler()\n",
        "X_train = sc.fit_transform(train.iloc[:, 1:-2])\n",
        "X_test = sc.transform(test.iloc[:, 1:-2])\n",
        "y_train = train.loc[:, \"labels\"].values\n",
        "y_test = test.loc[:, \"labels\"].values\n",
        "\n",
        "clf = RandomForestClassifier(max_depth=10, random_state=0)\n",
        "clf.fit(X_train, y_train)\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "metric_vals, _, _ = metrics(y_test, y_pred)\n",
        "metric_vals"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'acc': 0.6394422310756972, 'confusion_matrix': array([[232,  36,   0,   0],\n",
              "        [ 96,  89,   0,   0],\n",
              "        [  7,   5,   0,   0],\n",
              "        [ 35,   2,   0,   0]]), 'f1': 0.5951976436096736, 'mcc': 0.31622713710618033, 'precision': 0.5832232902950033, 'recall': 0.5951976436096736}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BByA-lk1SSEj",
        "colab_type": "text"
      },
      "source": [
        "### Keras Implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVVA4XSyPkxC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, BatchNormalization\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import LearningRateScheduler, EarlyStopping\n",
        "\n",
        "#Hyperparamters\n",
        "DROPOUT = 0.2\n",
        "HIDDEN_SIZE = 100\n",
        "BATCH_SIZES = [8, 16, 32]\n",
        "NUM_LABELS = 4\n",
        "NUM_EPOCHS = 100\n",
        "EARLY_STOPPING = {\"patience\": 3, \"delta\": 0.005}\n",
        "LEARNING_RATES = [0.0001, 0.001, 0.01, 0.1]\n",
        "WEIGHT_DECAY = 0.1 \n",
        "WARMUP = 0.06 \n",
        "OUTPUT_DIR = \"/content/drive/My Drive/Dog_Whistle_Code/Fine_Tuned_Models/Multimodal/Feature Concatenation\"\n",
        "\n",
        "def decay(epoch, lr):\n",
        "    epochs_drop = 5\n",
        "    DECAY_RATE = 0.94\n",
        "    lrate = lr * (DECAY_RATE**((1+epoch)/epochs_drop))\n",
        "    return lrate\n",
        "\n",
        "SCHEDULER = LearningRateScheduler(decay)\n",
        "\n",
        "\n",
        "# Load data\n",
        "# train = pd.read_csv(\"/content/drive/My Drive/Dog_Whistle_Code/Data/Train/combined_features.csv\")\n",
        "# dev = pd.read_csv(\"/content/drive/My Drive/Dog_Whistle_Code/Data/Validation/combined_features.csv\")\n",
        "# test = pd.read_csv(\"/content/drive/My Drive/Dog_Whistle_Code/Data/Test/combined_features.csv\")\n",
        "\n",
        "train = pd.read_csv(\"/content/drive/My Drive/Dog_Whistle_Code/Data/Train/combined_features_sabat.csv\")\n",
        "dev = pd.read_csv(\"/content/drive/My Drive/Dog_Whistle_Code/Data/Validation/combined_features_sabat.csv\")\n",
        "test = pd.read_csv(\"/content/drive/My Drive/Dog_Whistle_Code/Data/Test/combined_features_sabat.csv\")\n",
        "\n",
        "# Divide labels and features\n",
        "x_train = train.iloc[:, 1:-2]\n",
        "y_train = pd.get_dummies(train.loc[:, \"labels\"])\n",
        "x_dev = dev.iloc[:, 1:-2]\n",
        "y_dev = pd.get_dummies(dev.loc[:, \"labels\"])\n",
        "x_test = test.iloc[:, 1:-2]\n",
        "y_test = test.loc[:, \"labels\"].values.tolist() \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gi-YP3GYLWL9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def construct_model(MLP_type, hidden_size: int=50, dropout: float=0.2, num_labels: int=4, input_len: int = 5120):\n",
        "        \"\"\"Builds the network structure\n",
        "        image_model: CovNet from Keras library to use as image feature extractor\n",
        "\n",
        "        text_model: Transformer Model from Hugging Face to use as the text feature extractor\n",
        "\n",
        "        hidden_size (int): Number of nodes in the hidden layer. Defaulted to 50. \n",
        "\n",
        "        dropout (float): Rate at which nodes are deactivated. Defaulted to 0.2. \n",
        "        \n",
        "        num_labels (int): Number of labels to predict. Defaulted to 4.\n",
        "\n",
        "        input_len (int): Length of input vector. Defaulted to 5120 (Text feature length (4096) + image feature length (1024)).\n",
        "        \"\"\"\n",
        "\n",
        "        if MLP_type == \"Sabat\":\n",
        "            model = Sequential()\n",
        "            model.add(Dense(units=hidden_size, activation='relu',input_dim=input_len))\n",
        "            model.add(Dropout(0.2))\n",
        "            model.add(Dense(units=hidden_size, activation='relu',input_dim=hidden_size))\n",
        "            #model.add(Dropout(0.2))\n",
        "            model.add(Dense(units=num_labels, activation='softmax', input_dim=hidden_size))\n",
        "\n",
        "        if MLP_type == \"Gomez\":\n",
        "            model = Sequential()\n",
        "            model.add(Dense(units=input_len, activation='relu',input_dim=input_len))\n",
        "            model.add(BatchNormalization())\n",
        "            #model.add(Dropout(0.2))\n",
        "            model.add(Dense(units=1024, activation='relu',input_dim=input_len))\n",
        "            model.add(BatchNormalization())\n",
        "            #model.add(Dropout(0.2))\n",
        "            model.add(Dense(units=512, activation='relu',input_dim=1024))\n",
        "            model.add(BatchNormalization())\n",
        "            #model.add(Dropout(0.2))\n",
        "            model.add(Dense(units=num_labels, activation='softmax', input_dim=512))\n",
        "            \n",
        "        return model\n",
        "\n",
        "\n",
        "def model_trainer(input_model, x_train, x_test, x_dev, y_dev, early_stop_vals: dict, scheduler, epochs: int = 25, learning_rate: float = 1e-5, batch_size: int=8):   \n",
        "    \"\"\"\n",
        "    Trains multimodal model\n",
        "\n",
        "    input_model: Instatiation of model\n",
        "\n",
        "    x_train: Dataframe containing train features\n",
        "\n",
        "    y_train: Pandas series containing train labels\n",
        "\n",
        "    x_dev: Dataframe containing validation features\n",
        "\n",
        "    y_dev: Pandas series containing validation labels\n",
        "\n",
        "    early_stopping: Dictionary containing patience value (int) and delta value (float). The patience determines the number of epochs to wait to achieve the given delta\n",
        "\n",
        "    epochs (int): Number of times to run through all batches. Default value is 25.\n",
        "\n",
        "    learning_rate (float): Default value is 1e-5.\n",
        "\n",
        "    batch_size (int): Number of examples to be passed through the model at a given time. Defaulted to 8.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    Early_Stop = EarlyStopping(monitor='val_loss', min_delta=early_stop_vals[\"delta\"], patience=early_stop_vals[\"patience\"], verbose=1, mode='auto')\n",
        "    opt = Adam(learning_rate=learning_rate)\n",
        "    input_model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=[\"accuracy\"])\n",
        "    history = input_model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size, verbose=1, validation_data=(x_dev, y_dev), callbacks=[Early_Stop, scheduler])\n",
        "\n",
        "    train_dict = {\"Train Accuracy\": history.history['accuracy'], \"Train Loss\": history.history['loss'], \"Val Accuracy\": history.history['val_accuracy'], \"Val Loss\": history.history['val_loss'] }\n",
        "\n",
        "    return input_model, train_dict\n",
        "\n",
        "\n",
        "def model_tester(input_model, x_test, y_test):\n",
        "    \"\"\"\n",
        "    Tests the model's performance based on a several metrics\n",
        "\n",
        "    input_model: Instatiation of model\n",
        "\n",
        "    x_test: Dataframe containing test features\n",
        "\n",
        "    y_test: Pandas series containing test labels\n",
        "    \"\"\"\n",
        "\n",
        "    print('Predicting labels for {} sentences...'.format(len(x_test)))\n",
        "\n",
        "    preds = input_model.predict(x_test)\n",
        "    results, labels, predictions = metrics(y_test, preds, argmax_needed=True)\n",
        "\n",
        "    return results, labels, predictions\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_IxeBrLRmCX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 471
        },
        "outputId": "d7568a3e-bfb1-450f-fa4e-363d4f63fdd8"
      },
      "source": [
        "# Run Gomez\n",
        "Keras_Classifier = construct_model(\"Gomez\", HIDDEN_SIZE, DROPOUT, NUM_LABELS, 2048)\n",
        "trained_model, train_dict = model_trainer(Keras_Classifier, x_train, x_test, x_dev, y_dev, EARLY_STOPPING, SCHEDULER, NUM_EPOCHS, LEARNING_RATES[0], 8)   \n",
        "results, labels, predictions = model_tester(trained_model, x_test, y_test) \n",
        "print(results)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 3998 samples, validate on 500 samples\n",
            "Epoch 1/100\n",
            "3998/3998 [==============================] - 6s 2ms/step - loss: 1.0360 - accuracy: 0.6656 - val_loss: 0.9143 - val_accuracy: 0.7160\n",
            "Epoch 2/100\n",
            "3998/3998 [==============================] - 6s 2ms/step - loss: 0.6725 - accuracy: 0.7749 - val_loss: 0.8357 - val_accuracy: 0.6960\n",
            "Epoch 3/100\n",
            "3998/3998 [==============================] - 6s 2ms/step - loss: 0.5401 - accuracy: 0.8139 - val_loss: 0.9185 - val_accuracy: 0.6580\n",
            "Epoch 4/100\n",
            "3998/3998 [==============================] - 6s 1ms/step - loss: 0.4676 - accuracy: 0.8407 - val_loss: 0.8274 - val_accuracy: 0.7400\n",
            "Epoch 5/100\n",
            "3998/3998 [==============================] - 6s 2ms/step - loss: 0.3854 - accuracy: 0.8622 - val_loss: 0.8945 - val_accuracy: 0.7060\n",
            "Epoch 6/100\n",
            "3998/3998 [==============================] - 6s 1ms/step - loss: 0.3435 - accuracy: 0.8744 - val_loss: 0.9539 - val_accuracy: 0.7200\n",
            "Epoch 7/100\n",
            "3998/3998 [==============================] - 6s 2ms/step - loss: 0.2992 - accuracy: 0.8932 - val_loss: 0.8649 - val_accuracy: 0.7120\n",
            "Epoch 8/100\n",
            "3998/3998 [==============================] - 6s 1ms/step - loss: 0.2296 - accuracy: 0.9252 - val_loss: 0.8954 - val_accuracy: 0.7300\n",
            "Epoch 9/100\n",
            "3998/3998 [==============================] - 6s 1ms/step - loss: 0.2124 - accuracy: 0.9290 - val_loss: 0.9023 - val_accuracy: 0.7500\n",
            "Epoch 00009: early stopping\n",
            "Predicting labels for 502 sentences...\n",
            "{'mcc': 0.452972250453079, 'acc': 0.6912350597609562, 'confusion_matrix': array([[197,  51,   3,  17],\n",
            "       [ 40, 142,   0,   3],\n",
            "       [  4,   4,   4,   0],\n",
            "       [ 25,   7,   1,   4]]), 'precision': 0.6761401981635599, 'recall': 0.6912350597609562, 'f1': 0.6821803866213831}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJ885qcGh_Nb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 609
        },
        "outputId": "db0d3fe0-bd97-4fcd-ecad-e5ed0e3c516a"
      },
      "source": [
        "# Run Sabat\n",
        "Keras_Classifier = construct_model(\"Sabat\", HIDDEN_SIZE, DROPOUT, NUM_LABELS, 2048)\n",
        "trained_model, train_dict = model_trainer(Keras_Classifier, x_train, x_test, x_dev, y_dev, EARLY_STOPPING, SCHEDULER, NUM_EPOCHS, LEARNING_RATES[0], 25) #Note: lr from paper was LEARNING_RATES[-1]   \n",
        "results, labels, predictions = model_tester(trained_model, x_test, y_test) \n",
        "print(results)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 3998 samples, validate on 500 samples\n",
            "Epoch 1/100\n",
            "3998/3998 [==============================] - 1s 184us/step - loss: 1.3549 - accuracy: 0.6076 - val_loss: 0.9526 - val_accuracy: 0.6920\n",
            "Epoch 2/100\n",
            "3998/3998 [==============================] - 1s 156us/step - loss: 0.9460 - accuracy: 0.6846 - val_loss: 0.8377 - val_accuracy: 0.7220\n",
            "Epoch 3/100\n",
            "3998/3998 [==============================] - 1s 165us/step - loss: 0.7704 - accuracy: 0.7304 - val_loss: 0.8119 - val_accuracy: 0.7180\n",
            "Epoch 4/100\n",
            "3998/3998 [==============================] - 1s 159us/step - loss: 0.7276 - accuracy: 0.7361 - val_loss: 0.7848 - val_accuracy: 0.7280\n",
            "Epoch 5/100\n",
            "3998/3998 [==============================] - 1s 157us/step - loss: 0.6723 - accuracy: 0.7526 - val_loss: 0.7817 - val_accuracy: 0.7260\n",
            "Epoch 6/100\n",
            "3998/3998 [==============================] - 1s 158us/step - loss: 0.6394 - accuracy: 0.7699 - val_loss: 0.7674 - val_accuracy: 0.7460\n",
            "Epoch 7/100\n",
            "3998/3998 [==============================] - 1s 154us/step - loss: 0.6030 - accuracy: 0.7766 - val_loss: 0.7821 - val_accuracy: 0.7320\n",
            "Epoch 8/100\n",
            "3998/3998 [==============================] - 1s 159us/step - loss: 0.5868 - accuracy: 0.7846 - val_loss: 0.7752 - val_accuracy: 0.7340\n",
            "Epoch 9/100\n",
            "3998/3998 [==============================] - 1s 160us/step - loss: 0.5674 - accuracy: 0.7896 - val_loss: 0.7413 - val_accuracy: 0.7440\n",
            "Epoch 10/100\n",
            "3998/3998 [==============================] - 1s 155us/step - loss: 0.5442 - accuracy: 0.7951 - val_loss: 0.7643 - val_accuracy: 0.7380\n",
            "Epoch 11/100\n",
            "3998/3998 [==============================] - 1s 165us/step - loss: 0.5257 - accuracy: 0.8062 - val_loss: 0.7705 - val_accuracy: 0.7380\n",
            "Epoch 12/100\n",
            "3998/3998 [==============================] - 1s 157us/step - loss: 0.5166 - accuracy: 0.8037 - val_loss: 0.7715 - val_accuracy: 0.7500\n",
            "Epoch 00012: early stopping\n",
            "Predicting labels for 502 sentences...\n",
            "{'mcc': 0.4915644608084453, 'acc': 0.7250996015936255, 'confusion_matrix': array([[220,  47,   0,   1],\n",
            "       [ 42, 143,   0,   0],\n",
            "       [  6,   6,   0,   0],\n",
            "       [ 32,   4,   0,   1]]), 'precision': 0.6918492695883134, 'recall': 0.7250996015936255, 'f1': 0.6910988867108053}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_NRkddxP5h7f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "246c1ec2-d138-4172-ca8f-8c0858e944c1"
      },
      "source": [
        "results_dict = {}\n",
        "max_f1_value = 0\n",
        "\n",
        "for i in BATCH_SIZES:\n",
        "    learning_rate_dict = {}\n",
        "    for j in LEARNING_RATES: \n",
        "        Keras_Classifier = construct_model(\"Sabat\", HIDDEN_SIZE, DROPOUT, NUM_LABELS, 2048)\n",
        "        trained_model, train_dict = model_trainer(Keras_Classifier, x_train, x_test, x_dev, y_dev, EARLY_STOPPING, SCHEDULER, NUM_EPOCHS, j, i) \n",
        "        learning_rate_dict[j], labels, predictions = model_tester(trained_model, x_test, y_test) \n",
        "\n",
        "    if learning_rate_dict[j][\"f1\"] >= max_f1_value: #only save best model\n",
        "        max_f1_value = learning_rate_dict[j][\"f1\"]\n",
        "        print(\"The new top F1 score is: {}. Saving model...\".format(max_f1_value))\n",
        "        model_saver(trained_model, \"Sabat\", \"Keras\", OUTPUT_DIR, train_dict, labels, predictions, test.loc[:, \"ids\"].values.tolist(), learning_rate_dict[j])\n",
        "\n",
        "    results_dict[i] = learning_rate_dict \n",
        "\n",
        "#save complete training results\n",
        "np.save(os.path.join(os.path.join(OUTPUT_DIR, \"Sabat\"), \"dogwhistle_total_training_results_sabat.npy\"), results_dict)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 3998 samples, validate on 500 samples\n",
            "Epoch 1/100\n",
            "3998/3998 [==============================] - 3s 710us/step - loss: 1.1357 - accuracy: 0.6426 - val_loss: 0.8626 - val_accuracy: 0.6920\n",
            "Epoch 2/100\n",
            "3998/3998 [==============================] - 3s 633us/step - loss: 0.7803 - accuracy: 0.7261 - val_loss: 0.8412 - val_accuracy: 0.7060\n",
            "Epoch 3/100\n",
            "3998/3998 [==============================] - 3s 628us/step - loss: 0.6948 - accuracy: 0.7474 - val_loss: 0.7838 - val_accuracy: 0.7260\n",
            "Epoch 4/100\n",
            "3998/3998 [==============================] - 2s 617us/step - loss: 0.6571 - accuracy: 0.7599 - val_loss: 0.7899 - val_accuracy: 0.7260\n",
            "Epoch 5/100\n",
            "3998/3998 [==============================] - 2s 618us/step - loss: 0.6121 - accuracy: 0.7724 - val_loss: 0.7806 - val_accuracy: 0.7380\n",
            "Epoch 6/100\n",
            "3998/3998 [==============================] - 3s 649us/step - loss: 0.5819 - accuracy: 0.7806 - val_loss: 0.7882 - val_accuracy: 0.7480\n",
            "Epoch 00006: early stopping\n",
            "Predicting labels for 502 sentences...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 3998 samples, validate on 500 samples\n",
            "Epoch 1/100\n",
            "3998/3998 [==============================] - 3s 663us/step - loss: 0.9925 - accuracy: 0.6826 - val_loss: 0.7829 - val_accuracy: 0.7300\n",
            "Epoch 2/100\n",
            "3998/3998 [==============================] - 3s 636us/step - loss: 0.7417 - accuracy: 0.7376 - val_loss: 0.8225 - val_accuracy: 0.7300\n",
            "Epoch 3/100\n",
            "3998/3998 [==============================] - 3s 680us/step - loss: 0.6864 - accuracy: 0.7579 - val_loss: 0.7912 - val_accuracy: 0.7460\n",
            "Epoch 4/100\n",
            "3998/3998 [==============================] - 3s 640us/step - loss: 0.6687 - accuracy: 0.7559 - val_loss: 0.8025 - val_accuracy: 0.7440\n",
            "Epoch 00004: early stopping\n",
            "Predicting labels for 502 sentences...\n",
            "Train on 3998 samples, validate on 500 samples\n",
            "Epoch 1/100\n",
            "3998/3998 [==============================] - 3s 672us/step - loss: 1.7541 - accuracy: 0.6263 - val_loss: 0.8136 - val_accuracy: 0.7080\n",
            "Epoch 2/100\n",
            "3998/3998 [==============================] - 3s 639us/step - loss: 0.8310 - accuracy: 0.6596 - val_loss: 0.8704 - val_accuracy: 0.6260\n",
            "Epoch 3/100\n",
            "3998/3998 [==============================] - 3s 633us/step - loss: 0.8057 - accuracy: 0.6518 - val_loss: 0.7884 - val_accuracy: 0.6820\n",
            "Epoch 4/100\n",
            "3998/3998 [==============================] - 3s 638us/step - loss: 0.7817 - accuracy: 0.6926 - val_loss: 0.8654 - val_accuracy: 0.5380\n",
            "Epoch 5/100\n",
            "3998/3998 [==============================] - 3s 648us/step - loss: 0.7607 - accuracy: 0.6956 - val_loss: 0.7850 - val_accuracy: 0.7140\n",
            "Epoch 6/100\n",
            "3998/3998 [==============================] - 3s 658us/step - loss: 0.7425 - accuracy: 0.7046 - val_loss: 0.7687 - val_accuracy: 0.7220\n",
            "Epoch 7/100\n",
            "3998/3998 [==============================] - 3s 639us/step - loss: 0.7259 - accuracy: 0.7081 - val_loss: 0.7304 - val_accuracy: 0.7300\n",
            "Epoch 8/100\n",
            "3998/3998 [==============================] - 3s 657us/step - loss: 0.7141 - accuracy: 0.7174 - val_loss: 0.7745 - val_accuracy: 0.7260\n",
            "Epoch 9/100\n",
            "3998/3998 [==============================] - 3s 643us/step - loss: 0.6920 - accuracy: 0.7269 - val_loss: 0.7371 - val_accuracy: 0.7400\n",
            "Epoch 10/100\n",
            "3998/3998 [==============================] - 2s 624us/step - loss: 0.6897 - accuracy: 0.7346 - val_loss: 0.7789 - val_accuracy: 0.6960\n",
            "Epoch 00010: early stopping\n",
            "Predicting labels for 502 sentences...\n",
            "Train on 3998 samples, validate on 500 samples\n",
            "Epoch 1/100\n",
            "3998/3998 [==============================] - 3s 655us/step - loss: 18.1202 - accuracy: 0.4935 - val_loss: 0.9929 - val_accuracy: 0.5360\n",
            "Epoch 2/100\n",
            "3998/3998 [==============================] - 3s 633us/step - loss: 0.9871 - accuracy: 0.5215 - val_loss: 1.0070 - val_accuracy: 0.3700\n",
            "Epoch 3/100\n",
            "3998/3998 [==============================] - 3s 659us/step - loss: 1.1496 - accuracy: 0.5238 - val_loss: 0.9832 - val_accuracy: 0.5360\n",
            "Epoch 4/100\n",
            "3998/3998 [==============================] - 2s 617us/step - loss: 0.9918 - accuracy: 0.5283 - val_loss: 0.9913 - val_accuracy: 0.5360\n",
            "Epoch 5/100\n",
            "3998/3998 [==============================] - 3s 642us/step - loss: 0.9888 - accuracy: 0.5343 - val_loss: 0.9862 - val_accuracy: 0.5360\n",
            "Epoch 6/100\n",
            "3998/3998 [==============================] - 3s 662us/step - loss: 0.9865 - accuracy: 0.5225 - val_loss: 1.0055 - val_accuracy: 0.5360\n",
            "Epoch 00006: early stopping\n",
            "Predicting labels for 502 sentences...\n",
            "The new top F1 score is: 0.37607108933005484. Saving model...\n",
            "Saving complete.\n",
            "Train on 3998 samples, validate on 500 samples\n",
            "Epoch 1/100\n",
            "3998/3998 [==============================] - 1s 365us/step - loss: 1.3287 - accuracy: 0.6183 - val_loss: 0.9125 - val_accuracy: 0.6820\n",
            "Epoch 2/100\n",
            "3998/3998 [==============================] - 1s 330us/step - loss: 0.8598 - accuracy: 0.7041 - val_loss: 0.8351 - val_accuracy: 0.7180\n",
            "Epoch 3/100\n",
            "3998/3998 [==============================] - 1s 348us/step - loss: 0.7392 - accuracy: 0.7356 - val_loss: 0.8246 - val_accuracy: 0.7100\n",
            "Epoch 4/100\n",
            "3998/3998 [==============================] - 1s 334us/step - loss: 0.6731 - accuracy: 0.7579 - val_loss: 0.7931 - val_accuracy: 0.7280\n",
            "Epoch 5/100\n",
            "3998/3998 [==============================] - 1s 344us/step - loss: 0.6402 - accuracy: 0.7681 - val_loss: 0.7687 - val_accuracy: 0.7340\n",
            "Epoch 6/100\n",
            "3998/3998 [==============================] - 1s 328us/step - loss: 0.6124 - accuracy: 0.7706 - val_loss: 0.7603 - val_accuracy: 0.7420\n",
            "Epoch 7/100\n",
            "3998/3998 [==============================] - 1s 327us/step - loss: 0.5922 - accuracy: 0.7764 - val_loss: 0.7697 - val_accuracy: 0.7460\n",
            "Epoch 8/100\n",
            "3998/3998 [==============================] - 1s 336us/step - loss: 0.5609 - accuracy: 0.7924 - val_loss: 0.7686 - val_accuracy: 0.7440\n",
            "Epoch 9/100\n",
            "3998/3998 [==============================] - 1s 332us/step - loss: 0.5480 - accuracy: 0.7931 - val_loss: 0.7445 - val_accuracy: 0.7420\n",
            "Epoch 10/100\n",
            "3998/3998 [==============================] - 1s 329us/step - loss: 0.5308 - accuracy: 0.7984 - val_loss: 0.7357 - val_accuracy: 0.7500\n",
            "Epoch 11/100\n",
            "3998/3998 [==============================] - 1s 330us/step - loss: 0.5085 - accuracy: 0.8054 - val_loss: 0.7406 - val_accuracy: 0.7500\n",
            "Epoch 12/100\n",
            "3998/3998 [==============================] - 1s 326us/step - loss: 0.4962 - accuracy: 0.8054 - val_loss: 0.7437 - val_accuracy: 0.7480\n",
            "Epoch 13/100\n",
            "3998/3998 [==============================] - 1s 343us/step - loss: 0.4833 - accuracy: 0.8187 - val_loss: 0.7524 - val_accuracy: 0.7480\n",
            "Epoch 00013: early stopping\n",
            "Predicting labels for 502 sentences...\n",
            "Train on 3998 samples, validate on 500 samples\n",
            "Epoch 1/100\n",
            "3998/3998 [==============================] - 1s 373us/step - loss: 1.0036 - accuracy: 0.6866 - val_loss: 0.8215 - val_accuracy: 0.7140\n",
            "Epoch 2/100\n",
            "3998/3998 [==============================] - 1s 338us/step - loss: 0.7260 - accuracy: 0.7419 - val_loss: 0.7788 - val_accuracy: 0.7040\n",
            "Epoch 3/100\n",
            "3998/3998 [==============================] - 1s 343us/step - loss: 0.6837 - accuracy: 0.7574 - val_loss: 0.7600 - val_accuracy: 0.7320\n",
            "Epoch 4/100\n",
            "3998/3998 [==============================] - 1s 331us/step - loss: 0.6476 - accuracy: 0.7654 - val_loss: 0.7552 - val_accuracy: 0.7360\n",
            "Epoch 5/100\n",
            "3998/3998 [==============================] - 1s 340us/step - loss: 0.6224 - accuracy: 0.7741 - val_loss: 0.7923 - val_accuracy: 0.7380\n",
            "Epoch 6/100\n",
            "3998/3998 [==============================] - 1s 324us/step - loss: 0.5913 - accuracy: 0.7816 - val_loss: 0.7248 - val_accuracy: 0.7460\n",
            "Epoch 7/100\n",
            "3998/3998 [==============================] - 1s 325us/step - loss: 0.5738 - accuracy: 0.7899 - val_loss: 0.7395 - val_accuracy: 0.7360\n",
            "Epoch 8/100\n",
            "3998/3998 [==============================] - 1s 332us/step - loss: 0.5426 - accuracy: 0.7971 - val_loss: 0.7364 - val_accuracy: 0.7400\n",
            "Epoch 9/100\n",
            "3998/3998 [==============================] - 1s 327us/step - loss: 0.5271 - accuracy: 0.7969 - val_loss: 0.7856 - val_accuracy: 0.7360\n",
            "Epoch 00009: early stopping\n",
            "Predicting labels for 502 sentences...\n",
            "Train on 3998 samples, validate on 500 samples\n",
            "Epoch 1/100\n",
            "3998/3998 [==============================] - 1s 372us/step - loss: 1.6553 - accuracy: 0.6743 - val_loss: 0.9375 - val_accuracy: 0.5940\n",
            "Epoch 2/100\n",
            "3998/3998 [==============================] - 1s 342us/step - loss: 0.8784 - accuracy: 0.6821 - val_loss: 0.8312 - val_accuracy: 0.6980\n",
            "Epoch 3/100\n",
            "3998/3998 [==============================] - 1s 331us/step - loss: 0.7686 - accuracy: 0.7296 - val_loss: 0.8028 - val_accuracy: 0.7120\n",
            "Epoch 4/100\n",
            "3998/3998 [==============================] - 1s 338us/step - loss: 0.7408 - accuracy: 0.7364 - val_loss: 0.7888 - val_accuracy: 0.7240\n",
            "Epoch 5/100\n",
            "3998/3998 [==============================] - 1s 334us/step - loss: 0.7084 - accuracy: 0.7571 - val_loss: 0.7757 - val_accuracy: 0.7360\n",
            "Epoch 6/100\n",
            "3998/3998 [==============================] - 1s 359us/step - loss: 0.7089 - accuracy: 0.7549 - val_loss: 0.7503 - val_accuracy: 0.7520\n",
            "Epoch 7/100\n",
            "3998/3998 [==============================] - 1s 334us/step - loss: 0.7121 - accuracy: 0.7466 - val_loss: 0.8116 - val_accuracy: 0.7420\n",
            "Epoch 8/100\n",
            "3998/3998 [==============================] - 1s 345us/step - loss: 0.6809 - accuracy: 0.7669 - val_loss: 0.8043 - val_accuracy: 0.7560\n",
            "Epoch 9/100\n",
            "3998/3998 [==============================] - 1s 348us/step - loss: 0.6767 - accuracy: 0.7716 - val_loss: 0.7261 - val_accuracy: 0.7540\n",
            "Epoch 10/100\n",
            "3998/3998 [==============================] - 1s 330us/step - loss: 0.6697 - accuracy: 0.7741 - val_loss: 0.7386 - val_accuracy: 0.7580\n",
            "Epoch 11/100\n",
            "3998/3998 [==============================] - 1s 329us/step - loss: 0.6599 - accuracy: 0.7744 - val_loss: 0.7456 - val_accuracy: 0.7420\n",
            "Epoch 12/100\n",
            "3998/3998 [==============================] - 1s 331us/step - loss: 0.6454 - accuracy: 0.7861 - val_loss: 0.7745 - val_accuracy: 0.7400\n",
            "Epoch 00012: early stopping\n",
            "Predicting labels for 502 sentences...\n",
            "Train on 3998 samples, validate on 500 samples\n",
            "Epoch 1/100\n",
            "3998/3998 [==============================] - 1s 363us/step - loss: 12.7997 - accuracy: 0.5083 - val_loss: 1.0292 - val_accuracy: 0.5340\n",
            "Epoch 2/100\n",
            "3998/3998 [==============================] - 1s 332us/step - loss: 2.1130 - accuracy: 0.5193 - val_loss: 0.9823 - val_accuracy: 0.5360\n",
            "Epoch 3/100\n",
            "3998/3998 [==============================] - 1s 338us/step - loss: 1.0067 - accuracy: 0.5233 - val_loss: 0.9764 - val_accuracy: 0.5360\n",
            "Epoch 4/100\n",
            "3998/3998 [==============================] - 1s 335us/step - loss: 0.9651 - accuracy: 0.5485 - val_loss: 0.9731 - val_accuracy: 0.5360\n",
            "Epoch 5/100\n",
            "3998/3998 [==============================] - 1s 323us/step - loss: 0.9916 - accuracy: 0.5213 - val_loss: 0.9827 - val_accuracy: 0.5360\n",
            "Epoch 6/100\n",
            "3998/3998 [==============================] - 1s 323us/step - loss: 0.9890 - accuracy: 0.5278 - val_loss: 0.9815 - val_accuracy: 0.5360\n",
            "Epoch 00006: early stopping\n",
            "Predicting labels for 502 sentences...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:900: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 3998 samples, validate on 500 samples\n",
            "Epoch 1/100\n",
            "3998/3998 [==============================] - 1s 208us/step - loss: 1.8474 - accuracy: 0.5875 - val_loss: 1.0201 - val_accuracy: 0.6840\n",
            "Epoch 2/100\n",
            "3998/3998 [==============================] - 1s 194us/step - loss: 1.0118 - accuracy: 0.6771 - val_loss: 0.9126 - val_accuracy: 0.6960\n",
            "Epoch 3/100\n",
            "3998/3998 [==============================] - 1s 194us/step - loss: 0.8192 - accuracy: 0.7144 - val_loss: 0.8527 - val_accuracy: 0.7120\n",
            "Epoch 4/100\n",
            "3998/3998 [==============================] - 1s 188us/step - loss: 0.7539 - accuracy: 0.7284 - val_loss: 0.8445 - val_accuracy: 0.7100\n",
            "Epoch 5/100\n",
            "3998/3998 [==============================] - 1s 191us/step - loss: 0.6848 - accuracy: 0.7499 - val_loss: 0.8362 - val_accuracy: 0.6980\n",
            "Epoch 6/100\n",
            "3998/3998 [==============================] - 1s 192us/step - loss: 0.6466 - accuracy: 0.7636 - val_loss: 0.8252 - val_accuracy: 0.7300\n",
            "Epoch 7/100\n",
            "3998/3998 [==============================] - 1s 190us/step - loss: 0.6361 - accuracy: 0.7679 - val_loss: 0.8145 - val_accuracy: 0.7080\n",
            "Epoch 8/100\n",
            "3998/3998 [==============================] - 1s 195us/step - loss: 0.6088 - accuracy: 0.7746 - val_loss: 0.8137 - val_accuracy: 0.7200\n",
            "Epoch 9/100\n",
            "3998/3998 [==============================] - 1s 183us/step - loss: 0.5888 - accuracy: 0.7804 - val_loss: 0.8287 - val_accuracy: 0.7220\n",
            "Epoch 10/100\n",
            "3998/3998 [==============================] - 1s 175us/step - loss: 0.5802 - accuracy: 0.7849 - val_loss: 0.8078 - val_accuracy: 0.7160\n",
            "Epoch 11/100\n",
            "3998/3998 [==============================] - 1s 177us/step - loss: 0.5517 - accuracy: 0.7874 - val_loss: 0.8165 - val_accuracy: 0.7260\n",
            "Epoch 12/100\n",
            "3998/3998 [==============================] - 1s 180us/step - loss: 0.5454 - accuracy: 0.7974 - val_loss: 0.8071 - val_accuracy: 0.7260\n",
            "Epoch 13/100\n",
            "3998/3998 [==============================] - 1s 179us/step - loss: 0.5350 - accuracy: 0.7969 - val_loss: 0.8078 - val_accuracy: 0.7220\n",
            "Epoch 00013: early stopping\n",
            "Predicting labels for 502 sentences...\n",
            "Train on 3998 samples, validate on 500 samples\n",
            "Epoch 1/100\n",
            "3998/3998 [==============================] - 1s 220us/step - loss: 1.0557 - accuracy: 0.6688 - val_loss: 0.8071 - val_accuracy: 0.7000\n",
            "Epoch 2/100\n",
            "3998/3998 [==============================] - 1s 184us/step - loss: 0.7214 - accuracy: 0.7334 - val_loss: 0.7879 - val_accuracy: 0.7340\n",
            "Epoch 3/100\n",
            "3998/3998 [==============================] - 1s 182us/step - loss: 0.6745 - accuracy: 0.7569 - val_loss: 0.7402 - val_accuracy: 0.7380\n",
            "Epoch 4/100\n",
            "3998/3998 [==============================] - 1s 188us/step - loss: 0.6476 - accuracy: 0.7621 - val_loss: 0.7317 - val_accuracy: 0.7420\n",
            "Epoch 5/100\n",
            "3998/3998 [==============================] - 1s 186us/step - loss: 0.6097 - accuracy: 0.7786 - val_loss: 0.7745 - val_accuracy: 0.7360\n",
            "Epoch 6/100\n",
            "3998/3998 [==============================] - 1s 181us/step - loss: 0.5909 - accuracy: 0.7826 - val_loss: 0.8200 - val_accuracy: 0.7480\n",
            "Epoch 7/100\n",
            "3998/3998 [==============================] - 1s 172us/step - loss: 0.5719 - accuracy: 0.7854 - val_loss: 0.7364 - val_accuracy: 0.7380\n",
            "Epoch 00007: early stopping\n",
            "Predicting labels for 502 sentences...\n",
            "Train on 3998 samples, validate on 500 samples\n",
            "Epoch 1/100\n",
            "3998/3998 [==============================] - 1s 234us/step - loss: 2.2517 - accuracy: 0.6466 - val_loss: 0.8425 - val_accuracy: 0.7280\n",
            "Epoch 2/100\n",
            "3998/3998 [==============================] - 1s 183us/step - loss: 0.7550 - accuracy: 0.7174 - val_loss: 0.7689 - val_accuracy: 0.7340\n",
            "Epoch 3/100\n",
            "3998/3998 [==============================] - 1s 189us/step - loss: 0.7225 - accuracy: 0.7329 - val_loss: 0.8610 - val_accuracy: 0.7340\n",
            "Epoch 4/100\n",
            "3998/3998 [==============================] - 1s 182us/step - loss: 0.7136 - accuracy: 0.7421 - val_loss: 0.7599 - val_accuracy: 0.7200\n",
            "Epoch 5/100\n",
            "3998/3998 [==============================] - 1s 187us/step - loss: 0.6846 - accuracy: 0.7501 - val_loss: 0.7928 - val_accuracy: 0.7020\n",
            "Epoch 6/100\n",
            "3998/3998 [==============================] - 1s 198us/step - loss: 0.6599 - accuracy: 0.7594 - val_loss: 0.7839 - val_accuracy: 0.7340\n",
            "Epoch 7/100\n",
            "3998/3998 [==============================] - 1s 187us/step - loss: 0.6268 - accuracy: 0.7719 - val_loss: 0.7448 - val_accuracy: 0.7560\n",
            "Epoch 8/100\n",
            "3998/3998 [==============================] - 1s 177us/step - loss: 0.6119 - accuracy: 0.7769 - val_loss: 0.7380 - val_accuracy: 0.7460\n",
            "Epoch 9/100\n",
            "3998/3998 [==============================] - 1s 195us/step - loss: 0.6121 - accuracy: 0.7681 - val_loss: 0.7514 - val_accuracy: 0.7560\n",
            "Epoch 10/100\n",
            "3998/3998 [==============================] - 1s 181us/step - loss: 0.5699 - accuracy: 0.7814 - val_loss: 0.6962 - val_accuracy: 0.7320\n",
            "Epoch 11/100\n",
            "3998/3998 [==============================] - 1s 183us/step - loss: 0.5774 - accuracy: 0.7776 - val_loss: 0.7700 - val_accuracy: 0.7560\n",
            "Epoch 12/100\n",
            "3998/3998 [==============================] - 1s 180us/step - loss: 0.5655 - accuracy: 0.7789 - val_loss: 0.8248 - val_accuracy: 0.7460\n",
            "Epoch 13/100\n",
            "3998/3998 [==============================] - 1s 186us/step - loss: 0.5421 - accuracy: 0.7871 - val_loss: 0.7488 - val_accuracy: 0.7420\n",
            "Epoch 00013: early stopping\n",
            "Predicting labels for 502 sentences...\n",
            "Train on 3998 samples, validate on 500 samples\n",
            "Epoch 1/100\n",
            "3998/3998 [==============================] - 1s 217us/step - loss: 65.6245 - accuracy: 0.5190 - val_loss: 0.9880 - val_accuracy: 0.5360\n",
            "Epoch 2/100\n",
            "3998/3998 [==============================] - 1s 178us/step - loss: 1.1323 - accuracy: 0.5260 - val_loss: 1.1212 - val_accuracy: 0.5340\n",
            "Epoch 3/100\n",
            "3998/3998 [==============================] - 1s 177us/step - loss: 0.9860 - accuracy: 0.5295 - val_loss: 1.1196 - val_accuracy: 0.5340\n",
            "Epoch 4/100\n",
            "3998/3998 [==============================] - 1s 179us/step - loss: 0.9842 - accuracy: 0.5343 - val_loss: 1.1222 - val_accuracy: 0.5340\n",
            "Epoch 00004: early stopping\n",
            "Predicting labels for 502 sentences...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kR_lqdIW8091",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "42d14608-b556-4a42-8101-ad6f154a0ac1"
      },
      "source": [
        "results_dict = {}\n",
        "max_f1_value = 0\n",
        "\n",
        "for i in BATCH_SIZES:\n",
        "    learning_rate_dict = {}\n",
        "    for j in LEARNING_RATES: \n",
        "        Keras_Classifier = construct_model(\"Gomez\", HIDDEN_SIZE, DROPOUT, NUM_LABELS, 2048)\n",
        "        trained_model, train_dict = model_trainer(Keras_Classifier, x_train, x_test, x_dev, y_dev, EARLY_STOPPING, SCHEDULER, NUM_EPOCHS, j, i) \n",
        "        learning_rate_dict[j], labels, predictions = model_tester(trained_model, x_test, y_test) \n",
        "\n",
        "    if learning_rate_dict[j][\"f1\"] >= max_f1_value: #only save best model\n",
        "        max_f1_value = learning_rate_dict[j][\"f1\"]\n",
        "        print(\"The new top F1 score is: {}. Saving model...\".format(max_f1_value))\n",
        "        model_saver(trained_model, \"Gomez\", \"Keras\", OUTPUT_DIR, train_dict, labels, predictions, test.loc[:, \"ids\"].values.tolist(), learning_rate_dict[j])\n",
        "\n",
        "    results_dict[i] = learning_rate_dict \n",
        "\n",
        "#save complete training results\n",
        "np.save(os.path.join(os.path.join(OUTPUT_DIR, \"Gomez\"), \"dogwhistle_total_training_results_gomez.npy\"), results_dict)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 3998 samples, validate on 500 samples\n",
            "Epoch 1/100\n",
            "3998/3998 [==============================] - 8s 2ms/step - loss: 1.1910 - accuracy: 0.5793 - val_loss: 0.9974 - val_accuracy: 0.6600\n",
            "Epoch 2/100\n",
            "3998/3998 [==============================] - 7s 2ms/step - loss: 0.8559 - accuracy: 0.7064 - val_loss: 0.8985 - val_accuracy: 0.6960\n",
            "Epoch 3/100\n",
            "3998/3998 [==============================] - 8s 2ms/step - loss: 0.7375 - accuracy: 0.7261 - val_loss: 0.8643 - val_accuracy: 0.6920\n",
            "Epoch 4/100\n",
            "3998/3998 [==============================] - 7s 2ms/step - loss: 0.6857 - accuracy: 0.7464 - val_loss: 0.8870 - val_accuracy: 0.6920\n",
            "Epoch 5/100\n",
            "3998/3998 [==============================] - 8s 2ms/step - loss: 0.6333 - accuracy: 0.7624 - val_loss: 0.7941 - val_accuracy: 0.7100\n",
            "Epoch 6/100\n",
            "3998/3998 [==============================] - 7s 2ms/step - loss: 0.6035 - accuracy: 0.7719 - val_loss: 0.9054 - val_accuracy: 0.6820\n",
            "Epoch 7/100\n",
            "3998/3998 [==============================] - 7s 2ms/step - loss: 0.5633 - accuracy: 0.7921 - val_loss: 0.8267 - val_accuracy: 0.6880\n",
            "Epoch 8/100\n",
            "3998/3998 [==============================] - 7s 2ms/step - loss: 0.5397 - accuracy: 0.7934 - val_loss: 0.8094 - val_accuracy: 0.7120\n",
            "Epoch 00008: early stopping\n",
            "Predicting labels for 502 sentences...\n",
            "Train on 3998 samples, validate on 500 samples\n",
            "Epoch 1/100\n",
            "3998/3998 [==============================] - 8s 2ms/step - loss: 0.9622 - accuracy: 0.6513 - val_loss: 0.7809 - val_accuracy: 0.7160\n",
            "Epoch 2/100\n",
            "3998/3998 [==============================] - 7s 2ms/step - loss: 0.8152 - accuracy: 0.7104 - val_loss: 0.8056 - val_accuracy: 0.7060\n",
            "Epoch 3/100\n",
            "3998/3998 [==============================] - 7s 2ms/step - loss: 0.7701 - accuracy: 0.7171 - val_loss: 0.8029 - val_accuracy: 0.6740\n",
            "Epoch 4/100\n",
            "3998/3998 [==============================] - 7s 2ms/step - loss: 0.7379 - accuracy: 0.7261 - val_loss: 0.7298 - val_accuracy: 0.7380\n",
            "Epoch 5/100\n",
            "3998/3998 [==============================] - 7s 2ms/step - loss: 0.7197 - accuracy: 0.7349 - val_loss: 0.7443 - val_accuracy: 0.7460\n",
            "Epoch 6/100\n",
            "3998/3998 [==============================] - 7s 2ms/step - loss: 0.6758 - accuracy: 0.7504 - val_loss: 0.7725 - val_accuracy: 0.7140\n",
            "Epoch 7/100\n",
            "3998/3998 [==============================] - 7s 2ms/step - loss: 0.6523 - accuracy: 0.7616 - val_loss: 0.7552 - val_accuracy: 0.7420\n",
            "Epoch 00007: early stopping\n",
            "Predicting labels for 502 sentences...\n",
            "Train on 3998 samples, validate on 500 samples\n",
            "Epoch 1/100\n",
            "3998/3998 [==============================] - 8s 2ms/step - loss: 1.2218 - accuracy: 0.6298 - val_loss: 1.1532 - val_accuracy: 0.7420\n",
            "Epoch 2/100\n",
            "3998/3998 [==============================] - 7s 2ms/step - loss: 0.8175 - accuracy: 0.7049 - val_loss: 0.7880 - val_accuracy: 0.7100\n",
            "Epoch 3/100\n",
            "3998/3998 [==============================] - 7s 2ms/step - loss: 0.7213 - accuracy: 0.7409 - val_loss: 0.8897 - val_accuracy: 0.7360\n",
            "Epoch 4/100\n",
            "3998/3998 [==============================] - 7s 2ms/step - loss: 0.6881 - accuracy: 0.7514 - val_loss: 0.7215 - val_accuracy: 0.7340\n",
            "Epoch 5/100\n",
            "3998/3998 [==============================] - 7s 2ms/step - loss: 0.6642 - accuracy: 0.7529 - val_loss: 0.8779 - val_accuracy: 0.7300\n",
            "Epoch 6/100\n",
            "3998/3998 [==============================] - 7s 2ms/step - loss: 0.6527 - accuracy: 0.7579 - val_loss: 0.9927 - val_accuracy: 0.7340\n",
            "Epoch 7/100\n",
            "3998/3998 [==============================] - 7s 2ms/step - loss: 0.6330 - accuracy: 0.7616 - val_loss: 0.7373 - val_accuracy: 0.7380\n",
            "Epoch 00007: early stopping\n",
            "Predicting labels for 502 sentences...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 3998 samples, validate on 500 samples\n",
            "Epoch 1/100\n",
            "3998/3998 [==============================] - 8s 2ms/step - loss: 1.6475 - accuracy: 0.6171 - val_loss: 1.6683 - val_accuracy: 0.7420\n",
            "Epoch 2/100\n",
            "3998/3998 [==============================] - 7s 2ms/step - loss: 1.0187 - accuracy: 0.6731 - val_loss: 3.9655 - val_accuracy: 0.7220\n",
            "Epoch 3/100\n",
            "3998/3998 [==============================] - 7s 2ms/step - loss: 1.3844 - accuracy: 0.6333 - val_loss: 326.0938 - val_accuracy: 0.4960\n",
            "Epoch 4/100\n",
            "3998/3998 [==============================] - 7s 2ms/step - loss: 1.2258 - accuracy: 0.6623 - val_loss: 200.1935 - val_accuracy: 0.4860\n",
            "Epoch 00004: early stopping\n",
            "Predicting labels for 502 sentences...\n",
            "The new top F1 score is: 0.42894325116573956. Saving model...\n",
            "Saving complete.\n",
            "Train on 3998 samples, validate on 500 samples\n",
            "Epoch 1/100\n",
            "3998/3998 [==============================] - 4s 1ms/step - loss: 1.1836 - accuracy: 0.5920 - val_loss: 1.1920 - val_accuracy: 0.6400\n",
            "Epoch 2/100\n",
            "3998/3998 [==============================] - 4s 1ms/step - loss: 0.7723 - accuracy: 0.7389 - val_loss: 0.8980 - val_accuracy: 0.7120\n",
            "Epoch 3/100\n",
            "3998/3998 [==============================] - 4s 983us/step - loss: 0.6600 - accuracy: 0.7764 - val_loss: 1.0181 - val_accuracy: 0.6640\n",
            "Epoch 4/100\n",
            "3998/3998 [==============================] - 4s 1ms/step - loss: 0.5536 - accuracy: 0.8064 - val_loss: 1.0046 - val_accuracy: 0.6720\n",
            "Epoch 5/100\n",
            "3998/3998 [==============================] - 4s 969us/step - loss: 0.4960 - accuracy: 0.8297 - val_loss: 1.0169 - val_accuracy: 0.6720\n",
            "Epoch 00005: early stopping\n",
            "Predicting labels for 502 sentences...\n",
            "Train on 3998 samples, validate on 500 samples\n",
            "Epoch 1/100\n",
            "3998/3998 [==============================] - 5s 1ms/step - loss: 0.9374 - accuracy: 0.6886 - val_loss: 0.8408 - val_accuracy: 0.7340\n",
            "Epoch 2/100\n",
            "3998/3998 [==============================] - 4s 999us/step - loss: 0.7245 - accuracy: 0.7424 - val_loss: 0.7792 - val_accuracy: 0.7240\n",
            "Epoch 3/100\n",
            "3998/3998 [==============================] - 4s 974us/step - loss: 0.7018 - accuracy: 0.7404 - val_loss: 0.8392 - val_accuracy: 0.6720\n",
            "Epoch 4/100\n",
            "3998/3998 [==============================] - 4s 971us/step - loss: 0.6343 - accuracy: 0.7591 - val_loss: 0.7622 - val_accuracy: 0.7380\n",
            "Epoch 5/100\n",
            "3998/3998 [==============================] - 4s 998us/step - loss: 0.5998 - accuracy: 0.7784 - val_loss: 0.8232 - val_accuracy: 0.7380\n",
            "Epoch 6/100\n",
            "3998/3998 [==============================] - 4s 969us/step - loss: 0.5700 - accuracy: 0.7784 - val_loss: 0.8968 - val_accuracy: 0.6880\n",
            "Epoch 7/100\n",
            "3998/3998 [==============================] - 4s 1ms/step - loss: 0.5518 - accuracy: 0.7894 - val_loss: 0.9220 - val_accuracy: 0.6860\n",
            "Epoch 00007: early stopping\n",
            "Predicting labels for 502 sentences...\n",
            "Train on 3998 samples, validate on 500 samples\n",
            "Epoch 1/100\n",
            "3998/3998 [==============================] - 4s 1ms/step - loss: 1.0839 - accuracy: 0.6613 - val_loss: 1.1963 - val_accuracy: 0.6900\n",
            "Epoch 2/100\n",
            "3998/3998 [==============================] - 4s 988us/step - loss: 0.8279 - accuracy: 0.7274 - val_loss: 1.2384 - val_accuracy: 0.6440\n",
            "Epoch 3/100\n",
            "3998/3998 [==============================] - 4s 974us/step - loss: 0.7196 - accuracy: 0.7536 - val_loss: 0.7542 - val_accuracy: 0.7560\n",
            "Epoch 4/100\n",
            "3998/3998 [==============================] - 4s 978us/step - loss: 0.6558 - accuracy: 0.7636 - val_loss: 0.7253 - val_accuracy: 0.7140\n",
            "Epoch 5/100\n",
            "3998/3998 [==============================] - 4s 955us/step - loss: 0.6186 - accuracy: 0.7701 - val_loss: 0.7518 - val_accuracy: 0.7120\n",
            "Epoch 6/100\n",
            "3998/3998 [==============================] - 4s 954us/step - loss: 0.5981 - accuracy: 0.7726 - val_loss: 0.7786 - val_accuracy: 0.7220\n",
            "Epoch 7/100\n",
            "3998/3998 [==============================] - 4s 966us/step - loss: 0.5475 - accuracy: 0.7891 - val_loss: 0.7528 - val_accuracy: 0.7380\n",
            "Epoch 00007: early stopping\n",
            "Predicting labels for 502 sentences...\n",
            "Train on 3998 samples, validate on 500 samples\n",
            "Epoch 1/100\n",
            "3998/3998 [==============================] - 4s 1ms/step - loss: 1.7491 - accuracy: 0.6403 - val_loss: 1.0430 - val_accuracy: 0.5600\n",
            "Epoch 2/100\n",
            "3998/3998 [==============================] - 4s 980us/step - loss: 0.8537 - accuracy: 0.7081 - val_loss: 0.9532 - val_accuracy: 0.6620\n",
            "Epoch 3/100\n",
            "3998/3998 [==============================] - 4s 967us/step - loss: 0.8701 - accuracy: 0.7109 - val_loss: 1.5582 - val_accuracy: 0.7120\n",
            "Epoch 4/100\n",
            "3998/3998 [==============================] - 4s 995us/step - loss: 0.8620 - accuracy: 0.7226 - val_loss: 14.1985 - val_accuracy: 0.5000\n",
            "Epoch 5/100\n",
            "3998/3998 [==============================] - 4s 984us/step - loss: 0.8114 - accuracy: 0.7301 - val_loss: 4.8453 - val_accuracy: 0.6740\n",
            "Epoch 00005: early stopping\n",
            "Predicting labels for 502 sentences...\n",
            "The new top F1 score is: 0.6208773016994675. Saving model...\n",
            "Saving complete.\n",
            "Train on 3998 samples, validate on 500 samples\n",
            "Epoch 1/100\n",
            "3998/3998 [==============================] - 3s 636us/step - loss: 1.1891 - accuracy: 0.5923 - val_loss: 0.9856 - val_accuracy: 0.6960\n",
            "Epoch 2/100\n",
            "3998/3998 [==============================] - 2s 544us/step - loss: 0.6983 - accuracy: 0.7819 - val_loss: 1.0685 - val_accuracy: 0.6580\n",
            "Epoch 3/100\n",
            "3998/3998 [==============================] - 2s 500us/step - loss: 0.5445 - accuracy: 0.8219 - val_loss: 0.9257 - val_accuracy: 0.7260\n",
            "Epoch 4/100\n",
            "3998/3998 [==============================] - 2s 498us/step - loss: 0.4324 - accuracy: 0.8644 - val_loss: 0.9538 - val_accuracy: 0.6940\n",
            "Epoch 5/100\n",
            "3998/3998 [==============================] - 2s 497us/step - loss: 0.3384 - accuracy: 0.8929 - val_loss: 0.9968 - val_accuracy: 0.6800\n",
            "Epoch 6/100\n",
            "3998/3998 [==============================] - 2s 491us/step - loss: 0.2659 - accuracy: 0.9222 - val_loss: 0.9579 - val_accuracy: 0.7140\n",
            "Epoch 00006: early stopping\n",
            "Predicting labels for 502 sentences...\n",
            "Train on 3998 samples, validate on 500 samples\n",
            "Epoch 1/100\n",
            "3998/3998 [==============================] - 2s 620us/step - loss: 0.9707 - accuracy: 0.6901 - val_loss: 1.0992 - val_accuracy: 0.6820\n",
            "Epoch 2/100\n",
            "3998/3998 [==============================] - 2s 514us/step - loss: 0.6749 - accuracy: 0.7561 - val_loss: 0.8032 - val_accuracy: 0.7300\n",
            "Epoch 3/100\n",
            "3998/3998 [==============================] - 2s 523us/step - loss: 0.6267 - accuracy: 0.7704 - val_loss: 0.8105 - val_accuracy: 0.7380\n",
            "Epoch 4/100\n",
            "3998/3998 [==============================] - 2s 523us/step - loss: 0.5839 - accuracy: 0.7851 - val_loss: 0.8983 - val_accuracy: 0.7220\n",
            "Epoch 5/100\n",
            "3998/3998 [==============================] - 2s 524us/step - loss: 0.5349 - accuracy: 0.8007 - val_loss: 0.9616 - val_accuracy: 0.6640\n",
            "Epoch 00005: early stopping\n",
            "Predicting labels for 502 sentences...\n",
            "Train on 3998 samples, validate on 500 samples\n",
            "Epoch 1/100\n",
            "3998/3998 [==============================] - 3s 641us/step - loss: 1.1624 - accuracy: 0.6638 - val_loss: 1.1996 - val_accuracy: 0.7200\n",
            "Epoch 2/100\n",
            "3998/3998 [==============================] - 2s 507us/step - loss: 0.8047 - accuracy: 0.7309 - val_loss: 0.9764 - val_accuracy: 0.7220\n",
            "Epoch 3/100\n",
            "3998/3998 [==============================] - 2s 515us/step - loss: 0.7241 - accuracy: 0.7541 - val_loss: 0.8227 - val_accuracy: 0.7520\n",
            "Epoch 4/100\n",
            "3998/3998 [==============================] - 2s 505us/step - loss: 0.6527 - accuracy: 0.7756 - val_loss: 1.0414 - val_accuracy: 0.6880\n",
            "Epoch 5/100\n",
            "3998/3998 [==============================] - 2s 514us/step - loss: 0.6360 - accuracy: 0.7704 - val_loss: 0.7593 - val_accuracy: 0.7100\n",
            "Epoch 6/100\n",
            "3998/3998 [==============================] - 2s 495us/step - loss: 0.5381 - accuracy: 0.7979 - val_loss: 0.8831 - val_accuracy: 0.6900\n",
            "Epoch 7/100\n",
            "3998/3998 [==============================] - 2s 503us/step - loss: 0.5232 - accuracy: 0.7969 - val_loss: 0.9411 - val_accuracy: 0.7060\n",
            "Epoch 8/100\n",
            "3998/3998 [==============================] - 2s 510us/step - loss: 0.4676 - accuracy: 0.8199 - val_loss: 0.9514 - val_accuracy: 0.7060\n",
            "Epoch 00008: early stopping\n",
            "Predicting labels for 502 sentences...\n",
            "Train on 3998 samples, validate on 500 samples\n",
            "Epoch 1/100\n",
            "3998/3998 [==============================] - 3s 670us/step - loss: 2.3631 - accuracy: 0.6386 - val_loss: 1.0694 - val_accuracy: 0.7080\n",
            "Epoch 2/100\n",
            "3998/3998 [==============================] - 2s 506us/step - loss: 0.7542 - accuracy: 0.7451 - val_loss: 0.8318 - val_accuracy: 0.6920\n",
            "Epoch 3/100\n",
            "3998/3998 [==============================] - 2s 496us/step - loss: 0.6897 - accuracy: 0.7594 - val_loss: 0.9367 - val_accuracy: 0.7120\n",
            "Epoch 4/100\n",
            "3998/3998 [==============================] - 2s 516us/step - loss: 0.6592 - accuracy: 0.7759 - val_loss: 0.7981 - val_accuracy: 0.7060\n",
            "Epoch 5/100\n",
            "3998/3998 [==============================] - 2s 510us/step - loss: 0.6444 - accuracy: 0.7804 - val_loss: 0.8267 - val_accuracy: 0.7400\n",
            "Epoch 6/100\n",
            "3998/3998 [==============================] - 2s 519us/step - loss: 0.6529 - accuracy: 0.7804 - val_loss: 1.0212 - val_accuracy: 0.7300\n",
            "Epoch 7/100\n",
            "3998/3998 [==============================] - 2s 507us/step - loss: 0.6422 - accuracy: 0.7891 - val_loss: 0.8569 - val_accuracy: 0.7460\n",
            "Epoch 00007: early stopping\n",
            "Predicting labels for 502 sentences...\n",
            "The new top F1 score is: 0.6989702620582243. Saving model...\n",
            "Saving complete.\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}